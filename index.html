<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <title>ASI-401: Indoor Search Using Swarm of Drones</title>
  <link rel="icon" href="./favicon.ico" type="image/x-icon">

  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@shoelace-style/shoelace@2.16.0/cdn/themes/light.css" />
  <script type="module"
    src="https://cdn.jsdelivr.net/npm/@shoelace-style/shoelace@2.16.0/cdn/shoelace-autoloader.js"></script>

  <link rel="stylesheet" href="./index.css">

  <link rel="stylesheet" href="./components/left-nav-tab/left-nav-tab.css">

  <link rel="stylesheet" href="./components/team-member/team-member.css">
  <script type="module" src="./components/team-member/team-member.js"></script>

  <link rel="stylesheet" href="./components/table-of-content/table-of-content.css">

  <script type="module" src="./components/table-of-abbreviations/table-of-abbreviations.js"></script>
  
  <script type="module" src="./components/comparision-safmc/comparison-safmc.js"></script>

  <script type="module" src="./components/comparision-image-processing/comparison-image-processing.js"></script>
  
  <script type="module" src="./components/msba-configuration/msba-configuration.js"></script>

  <script type="module" src="./components/drone_platform_selection/drone_platform_selection.js"></script>

  <script type="module" src="./components/decision-matrix-pillar/decision-matrix-pillar.js"></script>

  <script type="module" src="./components/rc4-test-scores/rc4-test-scores.js"></script>

  <script type="module" src="./components/actual-test-scores/actual-test-scores.js"></script>

  <script type="module" src="./components/image/image-component.js"></script>

  <script type="module" src="components/image/image-full.js"></script>

  <script type="module" src="./components/video/video.js"></script>

  <link rel="stylesheet" href="./components/references/references.css">

  <link rel="stylesheet" href="./components/scroll-to-top/scroll-to-top.css">
  <script src="./components/scroll-to-top/scroll-to-top.js"></script>

  <script src="./components/table-component/table-component.js"></script>

  <link href="https://unpkg.com/gridjs/dist/theme/mermaid.min.css" rel="stylesheet" />
  
</head>

<body>

  <div class="sidenav">
    <ul type="none">
      <li><a href="#title"onclick="showSection('title')">Title</a></li>
      <li><a href="#acknowledgements"onclick="showSection('acknowledgements')">Acknowledgements</a></li>
      <li><a href="#table-of-abbreviation"onclick="showSection('table-of-abbreviation')">Table Of Abbreviations</a></li>
  
      <li>
        <a href="#section-header-1" onclick="showSection('section-header-1')">1. Singapore Amazing Flying Competition 2025</a>
        <ul type="none">
          <li><a href="#sub-section-1-header-1">1.1 Category E Mission</a></li>
          <li>
            <a href="#sub-section-1-header-2">1.2 Playing Field</a>
            <ul type="none">
              <li><a href="#sub-section-1-header-2-header-1">1.2.1 Known Search Area</a></li>
              <li><a href="#sub-section-1-header-2-header-2">1.2.2 Unknown Search Area</a></li>
              <li><a href="#sub-section-1-header-2-header-3">1.2.3 Pillar Area</a></li>
            </ul>
          </li>
          <li><a href="#sub-section-1-header-3">1.3 Victim Marker</a></li>
          <li><a href="#sub-section-1-header-4">1.4 Danger Zone</a></li>
          <li><a href="#sub-section-1-header-5">1.5 Navigation Aid</a></li>
          <li><a href="#sub-section-1-header-6">1.6 Pillar Obstacle</a></li>
          <li><a href="#sub-section-1-header-7">1.7 SAFMC 2025 vs SAFMC 2024</a></li>
          <li><a href="#sub-section-1-header-8">1.8 Advantages and Disadvantages of SAFMC 2024's Strategy</a></li>
        </ul>
      </li>
  
      <li>
        <a href="#section-header-2" onclick="showSection('section-header-2')">2. Selected Drone Platform</a>
        <ul type="none">
          <li><a href="#sub-section-2-header-1">2.1 Hardware: Bitcraze Crazyflie</a></li>
          <li><a href="#sub-section-2-header-2">2.2 Drone Software and Development</a></li>
        </ul>
      </li>
  
  
      <li>
          <a href="#section-header-3" onclick="showSection('section-header-3')">3. Object Detection (Shu Hui)</a>
        <ul type="none">
          <li><a href="#sub-section-3-header-1">3.1 Object Selection</a></li>
          <li><a href="#sub-section-3-header-2">3.2 Onboard Object Detection</a></li>
          <li><a href="#sub-section-3-header-3">3.3 Offboard Object Detection</a></li>
          <li><a href="#sub-section-3-header-4">3.4 Optimising Detection Parameters for the Drone</a></li>
          <li><a href="#sub-section-3-header-5">3.5 Upgrades to Firmware</a></li>
          <li><a href="#sub-section-3-header-6">3.6 External System Optimisation</a></li>
        </ul>
      </li>

      <li>
        <a href="#section-header-4" onclick="showSection('section-header-4')">4. Mission Strategy</a>
        <ul type="none">
          <li><a href="#sub-section-4-header-1">4.1 Search Strategy</a></li>
          <li><a href="#sub-section-4-header-2">4.2 Object Detection</a></li>
          <li><a href="#sub-section-4-header-3">4.3 Navigation Aids</a></li>
        </ul>
      </li>

      <li>
        <a href="#section-header-5" onclick="showSection('section-header-5')">5. Known Search Area (Yan Yew)</a>
        <ul type="none">
          <li><a href="#sub-section-5-header-1">5.1 Selection of Search Algorithms</a></li>
          <li><a href="#sub-section-5-header-2">5.2 Modified Swarm Bug Algorithm</a></li>
          <li><a href="#sub-section-5-header-3">5.3 Initial Proposal Strategy Using MSBA</a></li>
          <li><a href="#sub-section-5-header-4">5.4 Simulation to Evaluate Search Strategy</a></li>
          <li><a href="#sub-section-5-header-5">5.5 Improvements to MSBA</a></li>
          <li><a href="#sub-section-5-header-6">5.6 Real Life Testing</a></li>
          <li><a href="#sub-section-5-header-7">5.7 Final MSBA Parameters</a></li>

        </ul>
      </li>

      <li>
        <a href="#section-header-6" onclick="showSection('section-header-6')">6. Unknown Search Area (Yan Yew)</a>
        <ul type="none">
          <li><a href="#sub-section-6-header-1">6.1 Feasibility Testing of Simulatanous Mapping and Localisation</a></li>
            <li>
            <a href="#sub-section-6-header-2">6.2 Implementation of MSBA</a>
            <ul type="none">
              <li><a href="#sub-section-6-header-2-header-1">6.2.1 Drone Guidance to the Unknown Search Area</a></li>
              <li><a href="#sub-section-6-header-2-header-2">6.2.2 MSBA for Searching the Unknown Search Area</a></li>
            </ul>
          </li>     
          <li><a href="#sub-section-6-header-3">6.3 Integration with Mission Planner</a></li>
          <li><a href="#sub-section-6-header-4">6.4 Real Life Testing</a></li>

        </ul>
      </li>
  
      <li>
        <a href="#section-header-7" onclick="showSection('section-header-7')">7.Pillar Area (Matthew)</a>
        <ul type="none">
          <li><a href="#sub-section-7-header-1">7.1 Introduction to the Pillar Area</a></li>
          <li><a href="#sub-section-7-header-2">7.2 Mock-Up of the Pillar Area</a></li>
          <li><a href="#sub-section-7-header-3">7.3 Testing of Search Strategies Within</a></li>
          <li><a href="#sub-section-7-header-4">7.4 Testing of Victim Placement</a></li>
          <li>
            <a href="#sub-section-7-header-5">7.5 Optimising for Search and Rescue</a>
            <ul type="none">
              <li><a href="#sub-section-7-header-5-header-1">7.5.1 Sliding to the Side When Near a Pillar</a></li>
              <li><a href="#sub-section-7-header-5-header-3">7.5.2 Optimising the speed of the drones</a></li>
              <li><a href="#sub-section-7-header-5-header-3">7.5.3 Navigation Aid to Reverse at the End of the Pillar Area</a></li>
              <li><a href="#sub-section-7-header-5-header-3">7.5.4 Straight line vs Zig-Zag path</a></li>
            </ul>
          </li> 
          <li><a href="#sub-section-7-header-6">7.6 Full Algorithm</a></li>
        </ul>
      </li>
  
      <li>
        <a href="#section-header-8" onclick="showSection('section-header-8')">8. Mission Planning (Shu Hui)</a>
        <ul type="none">
          <li><a href="#sub-section-8-header-1">8.1 Optimisation of Take-off Sequence</a></li>
          <li><a href="#sub-section-8-header-2">8.2 Optimisation of Kill and Landing Sequence</a></li>
          <li><a href="#sub-section-8-header-3">8.3 Communication Between the Ground Control Station and Drones</a></li>
        </ul>
      </li>
  
      <li>
        <a href="#section-header-9" onclick="showSection('section-header-9')">9. Full-Scale Systems Testing</a>
        <ul type="none">
          <li>
            <a href="#sub-section-9-header-1">9.1 Testing in Residential College 4 Multi-Purpose Sports Hall</a>
            <ul type="none">
              <li><a href="#sub-section-9-header-1-header-1">9.1.1 Network and Bandwidth Issues</a></li>
              <li><a href="#sub-section-9-header-1-header-2">9.1.2 Different Performance of the Drones in Different Conditions</a></li>
              <li><a href="#sub-section-9-header-1-header-3">9.1.3 Overall Analysis</a></li>
            </ul>
          </li>
          <li><a href="#sub-section-9-header-2">9.2 Competition Day</a></li>
        </ul>
      </li>
  
      <li>
        <a href="#section-header-10" onclick="showSection('section-header-10')">10. Conclusion and Future Work</a>
        <ul type="none">
          <li><a href="#sub-section-10-header-1">10.1 Expanding Drone Capabilities</a></li>
          <li><a href="#sub-section-10-header-2">10.2 Decentralized Distributed SLAM</a></li>
        </ul>
      </li>
  
      <li><a href="#appendices" onclick="showSection('appendices')">Appendix A: Decision Matrix for Selection of Drone Development Platform</a></li>

      <li><a href="#references" onclick="showSection('references')">References</a></li>
    </ul>
  </div>

  <div class="content">
    <div class = "title">
    <div id="title"></div>
      
      <image-component tag="image" source="assets/title_page/nus_logo.png" subtitle=" ">
      </image-component>

      <h1>CDE4301 Innovation & Design Capstone</h1>

      <h1>AY2024/2025 Semester 2</h1>

      <h1>ASI-401: Indoor Search Using Swarm of Drones</h1>

      <h1>Group Report</h1>
      </div>

      <div class="team-member-wrapper">

      <team-member avatar="assets//title_page/shuhui.jpeg" name="Foo Shu Hui" department="Engineering Science Programme" 
       matric_number="A0244238E">
      </team-member>

      <team-member avatar="assets//title_page/yanyew.jpg" name="Low Yan Yew" department="Mechanical Engineering"
        matric_number="A0245906Y">
      </team-member>

      <team-member avatar="assets//title_page/samuel.png" name="Govindanath Samuel Sudharsanan" department="Mechanical Engineering"
        matric_number="A0239048Y">
      </team-member>

      <team-member avatar="assets/title_page/matthew.png" name="Matthew Yip Tze Herng" department="Mechanical Engineering"
        matric_number="A0249201N">
      </team-member>

    </div>

    <br />
    <br />

    <div id="acknowledgements" class ="acknowledgements" >
      
      <h2>Acknowledgements</h2>
      
      <p>We would like to extend our heartfelt gratitude to everyone who has contributed to our project in one way or another. Your support has been invaluable in ensuring its smooth progress, and we could not have accomplished this without you.</p>
      <p><strong>Project Supervisors</strong></p>
      <ul>
        <li>Dr. Elliot Law, Project Supervisor</li>
        <li>Mr. Nicholas Chew, Co-Project Supervisor</li>
      </ul>
     <p><strong>Temasek Lab & SAFMC Team</strong></p>
     <ul>    
        <li>Mr. Liew Yung Jun, Temasek Lab Staff, SAFMC 2024 Team Member</li>
        <li>Mr. Jimmy Chiun, Temasek Lab Staff, Former SAFMC Team Member</li>
        <li>Mr. William Leong, Temasek Lab Staff, Former SAFMC Team Member</li>
     </ul>
     <p><strong>EDIC Staff</strong></p>
    <ul>  
      <li>Ms. Annie Tan</li>
      <li>Mr. Alvin Poh</li>
    </ul>
    <p><strong>Residential College 4 (RC4) Staff & Directors</strong></p>
    <ul>   
      <li>Dr. Naviyn Prabhu Balakrishnan, Director of Student Life</li>
      <li>Mr. Scormon Ho Rui Sheng, Director of Sports</li>
      <li>Ms. Chloe Siew Ying Ning, Director of Clubs and Societies</li>
      <li>Ms. Ngu Hui Tze, Admin Staff</li>
      <li>Ms. Loi Hwee Fang, Admin Staff</li>
      <li>Mr. Tan You Cheng, Admin Staff</li>
    </ul>
    </div>

    <br />
    <br />

    <div class="table-of-abbreviations">
      <h2>Table of Abbreviations</h2>
      <div id="table-of-abbreviation"></div>

    </div>
    
    <br />
    <br />

    <div id = "main-content">

      <div id="section-header-1" class="section" style="display: none;">
        <h2>1. Singapore Amazing Flying Machine Competition 2025</h2>
        <p>Our group participated in the Singapore Amazing Flying Machine Competition (SAFMC) 2025 Open Category (Category E). In this category, we designed an autonomous drone swarm to search and rescue for victims in a Playing Field.</p>

        <p>Drone swarms have been used in a variety of real-world implementations, including environmental mapping and search and rescue. Defence Science Organisation (DSO), the organiser of the competition, stands to gain technical know-how from organising this competition. This can be applied in their development of drone systems for defence.</p>
      

      <div id="sub-section-1-header-1">
        <h3>1.1 Category E Mission</h3>

        <p>
          The mission of SAFMC 2025 Category E is: <br/><br/>

          <i>Design a system of <b>10 to 25 drones</b> to navigate through an <b>indoor environment</b> and search for victims,
          using either a <b>centralised</b> or <b>de-centralised</b> <u>fully autonomous control system</u>. The system must possess
          <b>localization</b>, <b>obstacle sensing</b> and <b>obstacle avoidance</b> capabilities.</i>
        </p>
      </div>

      <div id="sub-section-1-header-2">
        <h3>1.2 Playing Field</h3>
        <p>
          Figure 1.1 shows how the Playing Field may look like, which is not drawn to scale. Furthermore, the Danger Zones, Regular Victims and Bonus Victims may not be placed exactly where it is depicted in Figure 1.1. Teams are given two runs for the challenge. There is a maximum of two simultaneous take-offs in each run. For each run, the placement of Danger Zones, Regular Victims and Bonus Victims may change.
        </p>

        <image-component tag="image" source="./assets/section-1-safmc2025/figure_1_1_SAFMC_2025_playing_field.png"
                         subtitle="Figure 1.1: Indoor Arena of SAFMC 2022"></image-component>
        <p>
           The Playing Field has a size of 20 m x 14 m. The Start Area (green) has a size of 20 m x 6 m. 
        </p>
        <p>
            The Start Area is where the Crazyflies will take off from to start the mission. There is no limit to the number of Navigation Aids that can be used within the Start Area.
        </p>
        <p>
            For our analysis, we split the Playing Field into three sections: (1) Known Search Area. (2) Unknown Search Area. (3) Pillar Area.
        </p>
      </div>

      <div id="sub-section-1-header-2-header-1">
        <h4>1.2.1 Known Search Area</h4>

        <image-component tag="image" source="assets\section-1-safmc2025\figure_1_2_known_search_area.png"
        subtitle="Figure 1.2: Known Search Area"></image-component>  

        <p>The Known Search Area have the dimensions of 20 m x 14 m, which is the largest area in the Playing Field.</p>

        <p>The Known Search Area (Figure 1.2) will have Inner Walls of 2 m thickness and 2 m height, as well as Danger Zones, Regular Victims and Bonus Victims. The Inner Wall-to-Inner Wall distance is at least 2 m. There is a maximum of Ten Navigation Aids that can be used within the Playing Field (exclusive of the Unknown Search Area).</p>
      </div>

      <div id="sub-section-1-header-2-header-2">
        <h4>1.2.2 Unknown Search Area</h4>

        <image-component tag="image" source="assets\section-1-safmc2025\figure_1_3_unknown_search_area.png"
        subtitle="Figure 1.3: UnKnown Search Area"></image-component>  

      <p>
        The Unknown Search Area have the size of 8 m x 8 m and is situated in the centre of the Playing Field.
      </p>

      <p>
        The Unknown Search Area (Figure 1.3) contains Danger Zones, Inner Walls and Bonus Victims. There are two floor-to-ceiling entrances/exits to the Unknown Search Area.
      </p>

      <p>
        Navigation Aids are not allowed within the Unknown Search Area. The layout of the Unknown Search Area was not revealed on Competition Day.
      </p>
      </div>

      <div id="sub-section-1-header-2-header-3">
        <h4>1.2.3 Pillar Area</h4>

        <image-component tag="image" source="assets\section-1-safmc2025\figure_1_4_pillar_area.png"
        subtitle="Figure 1.4: Pillar Area"></image-component>

        <p>
          The Pillar Area (Figure 1.4) is the smallest area in the Playing Field.
        </p>
        <p>
          It consists of Eight pillars, with narrow paths (about 1 m wide) between the pillars. The Inner Wall-to-Pillar distance and the Pillar-to-Pillar distance is at least 1 m.
        </p>

      </div>

      <div id="sub-section-1-header-3">
        <h3>1.3 Victim Marker</h3>

        <image-component tag="image" source="assets\section-1-safmc2025\figure_1_5_specifications_of_victim_markers.png"
        subtitle="Figure 1.5: Specifications of Victim Markers"></image-component>

        <p>
          For SAFMC 2025, up to eight non-electronic markers was used as Victim Markers. The specifications of the Victim Markers must not exceed 30 cm x 30 cm x 1 m.
        </p>
        <p>
          Both Regular Victim and Bonus Victim share the same type of Victim Markers. Bonus Victims are situated in areas that are more challenging to navigate.
        </p>

        <image-component tag="image" source="assets\section-1-safmc2025\figure_1_6_scoring_process_rescue_victims.png"
        subtitle="Figure 1.6: Scoring Process (Rescue Victims)"></image-component>
        
        <p>
          To rescue a Victim, the Crazyflie needs to land within a 1 m radius of the Line of Sight (LOS) of the Regular Victim or a Bonus Victim (Figure 1.6). No obstacles are allowed within the Line of Sight.
          </p>
        
          <p>
          Each Victim Marker can be rescued only once by one Crazyflie.
          </p>

      </div>

      <div id="sub-section-1-header-4">
        <h3>1.4 Danger Zone</h3>

        <image-component tag="image" source="assets\section-1-safmc2025\figure_1_7_specifications_of_danger_zones.png"
        subtitle="Figure 1.7: Specifications of Danger Zones"></image-component>
        <p>
          For SAFMC 2025, up to four non-electronic markers was used as Danger Zones. The specifications of the Danger Zones must not exceed 30 cm x 30 cm x 1 m.
        </p>
        
        <image-component tag="image" source="assets\section-1-safmc2025\figure_1_8_penalty_process_avoid_danger_zones.png"
        subtitle="Figure 1.8: Penalty Process (Avoid Danger Zones)"></image-component>

        <p>
          Danger Zones may overlap with any Navigation Aids we place within the Playing Field.
        </p>
        <p>
          Crazyflies landing within a 1 m radius of the Danger Zone incurs a score penalty.
        </p>

      </div>

      <div id="sub-section-1-header-5">
        <h3>1.5 Navigation Aid</h3>

        <image-component tag="image" source="assets\section-1-safmc2025\figure_1_9_specifications_of_navigation_aids.png"
        subtitle="Figure 1.9: Specifications of Navigation Aids"></image-component>
        
        <p>
          For SAFMC 2025, a total of ten Navigation Aids was used within the Known Search Area and Pillar Area. The specifications of the Navigation Aids must not exceed 1 m x 1 m x an unspecified height. According to competition rule book, more than one type of Navigation Aids are allowed.
        </p>
      </div>

      <div id="sub-section-1-header-6">
        <h3>1.6 Pillar Obstacle</h3>

        <image-component tag="image" source="assets\section-1-safmc2025\figure_1_10_specifications_of_pillar_obstacles.png"
        subtitle="Figure 1.10: Specifications of Pillar Obstacles"></image-component>

        <p>
          Eight Pillar Obstacles was presented in the Pillar Area. Pillar Obstacles have the dimensions of 0.3 m diameter and 2 m height, inclusive of a weighted circular base of 0.5 m diameter and 0.15 m height.
        </p>
      
      </div>

      <div id="sub-section-1-header-7">
        <h3>1.7 SAFMC 2025 vs SAFMC 2024</h3>

        <comparison-safmc subtitle="Table 1.1: Comparison of Competition Rules for SAFMC 2024 and SAFMC 2025">
          <div id="comparison-safmc"></div>
        </comparison-safmc>

        <p>
          Table 1.1 shows an overview of the differences in the Competition Rules for SAFMC 2024 and SAFMC 2025.
        </p>
        <p>
          For SAFMC 2025, an Unknown Search Area is introduced, in which the layout of the Unknown Search Area will remain undisclosed throughout the Competition Day Mission. Additionally, the layout of the Known Search Area will only be disclosed on the Competition Day Mission. In contrast, for SAFMC 2024, the layout of the entire Arena was only disclosed on the Competition Day Mission.
          </p>
        
          <p>
          Furthermore, SAFMC 2025 introduces eight Pillar Obstacles, which are a new type of static obstacles within the Arena. Eight Victims Markers will be present, which includes both Regular Victims and Bonus Victims. Each type of Victim Markers offers different scoring opportunities. The Double-Rescue Victims are removed from this year’s competition.
          </p>
          
        <p>
          Additionally, a maximum of four Danger Zones is introduced for SAFMC 2025. Ten Navigation Aids are allowed within the Known Search Area and no Navigation Aids are allowed within the Unknown Search Area.
          </p>

      </div>

      <div id="sub-section-1-header-8">
        <h3>1.8 Advantages and Limitation of SAFMC 2024’s Mission Strategy</h3>
        
        <p>
          The strategy employed by the SAFMC 2024 team represented a significant shift towards from a centralised strategy (Optimal Reciprocal Collision Avoidance Algorithm) to a decentralized search strategy (Modified Swarm Gradient Bug Algorithm), in areas concerning navigation and collision avoidance, aiming to address limitations encountered by the SAFMC 2023 team.
        </p>
        <p>
          The Bug Algorithm was implemented onboard on the crazyflie firmware. This approach alleviated computational demands on the GCS, compared to the SAFMC 2023 team, which required multiple Navigation Aids for localisation (SAFMC 2023 competition had no limits on the number of Navigation Aids that can be used.) The navigation was more reliable, even if communication was lost.
        </p>
        <p>
          Furthermore, the team retained the GCS based AprilTag detection mechanism. However, this strategy had limitations. Victim detection and the issuance of landing commands remained centralized, which is dependent on Wi-Fi image streaming to the GCS. This dependency constituted a bottleneck and a single point of failure for the primary mission objective.
        </p>
        <p>
          Additionally, the Bug Algorithm did not guarantee exhaustive area coverage and required specific mechanisms such as loop detection, re-heading to improve efficiency.
        </p>
        <p>
          For collision avoidance, the ”Two-Lane” method was implemented. This method, while offering basic separation, lacked adaptability and remained sensitive to localization inaccuracies, especially as flight time increased.
        </p>
        </div>
    </div>

    <br />
    <br />

      <div id="section-header-2" class="section" style="display: none;">
        <h2>2. Selected Drone Platform </h2>
          <p>
            Given the project’s scope, an off-the-shelf drone platform was used in this project as building custom drones is complex and impractical. The selection of the drone platform for autonomous search and rescue missions was based on size and weight, modularity, sensor capabilities, cost, and availability. The ideal platform should be ultra-lightweight to support scalability in swarm applications. It should also be highly modular in both hardware and software, featuring swappable parts for ease of maintenance and open-source software to provide flexibility in developing solutions. The drone should be equipped with sensor suites that enable localization, obstacle detection, and avoidance capabilities. 
          </p>
          <p>
            The drone platforms considered were DJI Tello, Bitcraze Crazyflie 2.1+, and DEXI Drone – Level III. The selection matrix for the drone development platform selection is in <i>Appendix A</i>.
          </p>
      <div id="sub-section-2-header-1">
        <h3>2.1 Hardware: BitCraze Crazyflie</h3>
        <p>
          The Bitcraze Crazyflie 2.1+ was selected as the development platform for this project due to its superior lightweight design, modularity, and extensive sensor compatibility. 
        </p>

        <image-component tag="image" source="assets\section-2-selected-drone-platform\figure_2_1_bitcraze_crazyflie.png"
        subtitle="Figure 2.1: Bitcraze Crazyflie 2.1+ Drone Development Platform"></image-component>
        <p>
          A key advantage of the Crazyflie is its open-source framework, which includes the Crazyflie firmware, the cflib Python library, and compatibility with ROS2 via the Crazyswarm2 package. The Crazyflie also supports onboard autonomy using the Crazyflie App Layer, allowing mission-specific algorithms to run directly on the drone without requiring continuous external communication.
        </p>
        <image-full tag="image" source="assets\section-2-selected-drone-platform\figure_2_2_expansion_decks.png"
        subtitle="Figure 2.2: Expansion Decks: (Left) AI Deck, (Middle) Flow Deck, (Right) Multi-ranger Deck"></image-full>
        <p>
          Bitcraze has an extensive ecosystem, providing various expansion decks that enhances its sensing and navigation capabilities. The hardware modularity of the Crazyflie is leveraged by incorporating specific expansion decks, each chosen to fulfil a critical functional requirement for autonomous flight and interaction:
        </p>
        <ul style="text-align: justify;">
            <li><strong>AI Deck (Figure 2.2, Left):</strong> 
              This deck serves two important purposes. Firstly, its onboard monochrome camera (HM01B0) is the primary sensor for visual object detection (identifying AprilTags representing Victim Markers, Danger Zones, or Navigation Aids). Secondly, its integrated ESP32 module provides essential Wi-Fi connectivity, enabling image streaming to the GCS for processing and facilitating command/telemetry exchange. Furthermore, the GAP8 MCU offers potential for future onboard image processing and supports decentralized strategies.
            </li>
            <li>
              <strong>Flow Deck v2 (Figure 2.2, Middle):</strong> 
              This deck is essential for localization. It integrates a Time-of-Flight (ToF) sensor for accurate height-above-ground measurements and a downward-facing optical flow sensor to estimate velocity relative to the floor texture. While susceptible to drift, particularly over non-textured surfaces or long durations (which is a factor influencing mission strategy design), it provides the fundamental state estimation necessary for basic navigation and control. Alternatives like relying solely on the IMU are inadequate due to rapid drift accumulation.</li>
            <li>
              <strong>Multi-ranger Deck (Figure 2.2, Right):</strong> This deck provides horizontal obstacle detection, which is essential for navigating unknown environments and avoiding collisions with static obstacles such as walls and pillars. Its five ToF sensors (front, back, left, right, up) enable avoidance manoeuvres. Relying solely on the forward-facing camera on AI Deck or downward-facing ToF sensor on the Flow Deck would not provide the sufficient and comprehensive, real-time environmental awareness needed for navigation.
            </li>
        </ul>
        <image-full tag="image" source="assets\section-2-selected-drone-platform\figure_2_3_configuration_of_upgraded_crazyflie_with_expansion_decks.png"
        subtitle="Figure 2.3: Configuration of Upgraded Crazyflie with Expansion Decks"></image-full>
        <p>
          This specific combination of decks provides the minimum necessary sensing suite: (1) Localization (Flow Deck), (2) Obstacle Avoidance (Multi-ranger Deck), and (4) Object detection and communication (AI Deck) integrated within the Crazyflie's size, weight, and power constraints. While other specialized decks exist (e.g., Loco Positioning System deck for Ultra-Wide Band positioning), they were deemed unsuitable due to cost, infrastructure requirements, or incompatibility with mission constraints (e.g., non-electronic Victim Markers and Danger Zones).
        </p>

      </div>

      <div id="sub-section-2-header-2">
        <h3>2.2 Drone Software and Development</h3>
        <p>
          Crazyflie runs on the Bitcraze Crazyflie firmware, which is written in C and is fully open source. It includes an app layer that allow users to add custom code directly to the Crazyflie, making it the entry point for implementing decentralized autonomous capabilities. The app layer contains a set of APIs such as the Deck API that allows scripts running onboard to access sensor data like Multi-ranger Deck’s distance data, odometry data from   the Flow Deck, etc. (Bitcraze, 2024)  
        </p>

        <p>
          For centralized solutions, the Crazyflie can be integrated with ROS2 using the Crazyswarm2 Python package. This enables high-level control for multi-drone coordination via cflib, Python library that facilitates communication and control through Crazyradio, a long-range open USB radio dongle. In addition, the Crazyswarm2 package also enable the Ground Control Station (GCS) to access Crazyflie sensor data through the various ROS2 topics.
          The development workstations are equipped with ROS2 Humble, running on Ubuntu 22.04 to support software development.
        </p>

      </div>
    </div>
   
    <br />
    <br />

      <div id="section-header-3" class="section" style="display: none;">
      <h2>3. Object Detection (Shu Hui)</h2>
      <p>
        The Object Detection Subsystem involves the detection of objects such as Victim Markers, Danger Zones and Navigation Aids. Upon the detection of Victim Markers, land commands are transmitted via CPX (Crazyflie Packet eXchange) from the GCS to the drones. Similarly, upon the detection of Navigation Aids, the respective predefined Search Strategies will be “activated” after receiving commands from the GCS. In contrast, ignore commands is transmitted upon the detection of Danger Zones.
      </p>

    <div id="sub-section-3-header-1">
      <h3>3.1 Object Selection</h3>

    <p>
      A decision matrix was created to aid the object selection decision (Table 3.1). Objects refer to Navigation Aids, Victim Markers and Danger Zones, weighing factors crucial for competition success under tight time constraints.
    </p>

      <image-full tag="image" source="./assets/section-3-obj-detection/table_3_1_decision matrix.png"
      subtitle="Table 3.1 : Decision Matrix for Target and Danger Zone Selection "></image-full>

      <p>
        Given the project timeline and the breadth of subsystems required (Navigation Aids, Search Strategies, Mission Planning), solutions demanding extensive development time or complex integration were heavily penalized. Reliability was weighted lowest at this initial selection stage, assuming implementation quality would determine final reliability. Depending on the target type, the approach for object detection varies significantly.
      </p>
      <p>
        One possible approach we want to investigate was using Convolutional Neural Networks (CNN) detect simple objects such as cones. While a CNN model can be trained successfully for object detection, its detection accuracy is heavily dependent on the environment of both the training and testing data. There was also limited training data consisting of images of the specific cones taken by the HM01B0 monochromatic camera. As the competition location was revealed very close to actual competition date, using cones as a target introduced additional complexity as well as reduced feasibility. From feedback provided by the SAFMC 2024 team, we noted that Bitcraze had a face detection example that used two classes of objects, controlled via a GCS. However, this example had not been tested onboard, and there was a need to expand the system to classify more than 10 different object classes (Bitcraze, n.d.) for our system needs.
      </p>
      <p>
        Another possible approach we want to investigate was using Bitcraze’s Loco Positioning Deck which uses Ultra-wideband system to detect Loco Positioning anchors. However, the Loco Positioning System is expensive (Bitcraze, n.d.), with maximum detection range of 8 m to 10 m indoors (Bitcraze, n.d.). The Loco Positioning System was not feasible for a 20 m x 20 m playing field, as it would require multiple Loco Positioning anchors (to act as Navigation Aids) scattered throughout the field. Additionally, competition rules states that Navigation Aids may be electronic but both Danger Zones and Victim Markers must not be electronic. With our Crazyflie drones already upgraded with three decks, the male header pins are too short to accommodate an additional deck.
      </p>
      <p>
        The object chosen is AprilTags (Figure 3.2). AprilTags are visual fiducial markers used in applications such as camera calibration. AprilTags are chosen because it is more accurate than other markers such as WhyCon (Robotics Knowledgebase, n.d.). They meet the non-electronic requirement for Victim Markers and Danger Zones, are cheap, easy to create/print, and reliable under varying lighting conditions (compared to simple colour/shape detection). It allows calculation of the exact position, orientation and identity of a marker relative to a camera. Furthermore, it allows the user to specify a list of markers to detect and have existing ROS support.
      </p>
      <p>
        AprilTags are used as Victim Markers for SAFMC 2024 and SAFMC 2025. For SAFMC 2025, they are markers for Danger Zones and Navigation Aids. An exploration of on-board and off-board detection of AprilTags is then conducted.
      </p>
      <image-full tag="image" source="./assets/section-3-obj-detection/figure_3_1_objects.png"
    subtitle="Figure 3.1: Regular/Bonus Victims, Danger Markers and Navigation Aids"></image-full>

      <p>
        Competition rules state that the Navigation Aids must have the maximum base dimensions of 1 m x 1 m with no specified height. As the height of the Navigation Aids was not specified, we chose to make Nav Aids thin floor markers. This decision is influenced by the observation that the Flow Deck is very sensitive to floor texture and lighting conditions. Increasing the height of the Navigation Aids makes the drone fly higher momentarily to accommodate the height of the Navigation Aid when it is detected, which can disrupt the velocity estimation and localisation accuracy of the Flow Deck. Furthermore, we avoided mounting Navigation Aids on wall as the detection distance is too small, and the Multi-ranger Deck will just detect the Navigation Aid as a wall and not trigger the desired drone behaviour.
      </p>
      <p>
        Our novelty lies not in using AprilTags themselves, but in developing the system logic to interpret the same marker type for three different purposes based on detected ID and trigger appropriate, distinct drone actions (land, ignore, change behaviour in the different areas), managing this within a large drone swarm.
      </p>
    </div>

    <div id="sub-section-3-header-2">
      <h3>3.2 Onboard Detection</h3>
      <p>
        The SAFMC 2024 team used an off-board object detection approach with one GCS. They faced significant challenges such as delayed image transmission for a swarm, network robustness issues, and repeated drone disconnections, even on the Competition Day itself.
      </p>
      <p>
        To overcome these limitations and achieve a fully autonomous swarm that complements the decentralised search approach, an onboard object detection approach, meaning onboard image processing of the detected Victim Markers, was explored. The goal was to process images and detect AprilTags directly on the drone, reducing Wi-Fi traffic to only essential data.
      </p>
      <p>
        A comparison of the advantages of on-board image processing and off-board image processing in Table 3.2.
      </p>
      
      <comparison-image-processing subtitle="Table 3.2: Advantages between On-board Processing and Off-board Processing (Bitcraze, n.d.)">
        <div id="comparison-image-processing"></div>
      </comparison-image-processing>    
    
      <p>
        The GAP8 on the AI Deck has a modifiable application layer, which can be modified to enable image streaming capabilities over Wi-Fi (Bitcraze, n.d.) and include AprilTag detection using an open-source repository (Bitcraze, n.d.). On-board image processing involves modifying the existing image streaming capability to include AprilTag detection capabilities. The GAP8 RISC-V GNU processor runs on the FreeRTOS Operating System, but the FreeRTOS is not Portable Operating System Interface (POSIX) compliant, meaning that standard POSIX functions for threading and mutex creation are unavailable.</p>
      <p>
        Since the AprilTag library relies on POSIX functions for threading and creating mutexes, a FreeRTOS-plus-POSIX wrapper was explored (FreeRTOS, n.d.). The FreeRTOS-Plus-POSIX wrapper implements a limited subset of the POSIX threading API, which allows us to develop a FreeRTOS application using POSIX like threading primitives. However, this wrapper only implements about 20% of the POSIX API, making it insufficient for the GAP8’s requirements, and ultimately, it did not work as intended. Attempts with alternative libraries yielded similar POSIX-dependency issues.
      </p>
      <p>
        The next step was to manually refactor all POSIX functions in the AprilTag library with their FreeRTOS equivalents.
      </p>
      <p>
        Unfortunately, after considerable time spent investigating on-board detection, it became clear that refactoring the code for this purpose had a low probability of success and would require significant additional effort. Given the tight project deadline, even successful on-board would necessitate a complete rewrite of the higher-level mission logic such as the handling of the Victim Marker detection, the ignoring of Danger Zone, and Navigation Aid differentiation algorithms would also require a complete rewrite, diverging significantly from the ROS2-based GCS framework used by previous teams and available support repositories.
      </p>
      <p>
        Therefore, despite the theoretical advantages (particularly scalability and reduced network load) and the desire to utilize the AI Deck's processing power more fully, the onboard detection approach was deemed infeasible under the project constraints. This limitation meant we had to focus on optimizing the off-board approach.
      </p>
    </div>

    <div id="sub-section-3-header-3">
        <h3>3.3 Offboard Detection</h3>
        <p>
          For off-board detection, four repositories were found online (TL-NUS-CFS, 2023, Rauch, 2025, Rauch, 2025 and CDE-4301-ASI-401, n.d.). <i>ai_deck_wrapper</i> enables image streaming and integrates with <i>apriltag_ros</i> which handles AprilTag detection. <i>apriltag_msgs</i> defines the message type for communication between the AI deck wrapper node and the AprilTag node. The primary repository we worked with is the MissionPlanner, which supports the implementation of our mission strategy.
        </p>
        <video-component 
        source="https://www.youtube.com/embed/No0juLGqdLo" 
        subtitle="Figure 3.2: Video of Object Detection for One Crazyflie">
    </video-component>
        <p>
          The test for object detection (Figure 3.2) was conducted in the Computer Vision room at Temasek Laboratories. In this test, the Crazyflie takes off to search for one Victim Marker autonomously. The Crazyflie flies over the Danger Zone (AprilTag in the middle of the room) and rescuing the victim (AprilTag at the end of the room). The Search Algorithm used in this test was Modified Swarm Bug Algorithm. The angle of the camera mount was 45°.
        </p>
        <p>
          This test confirms that the Crazyflie successfully maintains a distance within the 1 m radius of the Victim Marker, achieves a successful rescue by landing near the Victim Marker, and accurately detects the Danger Zone without landing within it.
        </p>
        <p>
          Our primary contribution within this off-board framework was modifying the MissionPlanner and associated logic to handle the specific requirements of SAFMC 2025: (1) Implementing logic to check the ID of a detected AprilTag against predefined lists of Victim, Danger Zone, and Navigation Aid IDs, (2) Sending the correct CPX command based on the identified object type (land for Victim, ignore/continue for Danger, execute specific behaviour for Navigation Aid) and (3) Integration with Search Strategies.
        </p>

    </div>

    <div id="sub-section-3-header-4">
      <h3>3.4 Optimising Detection Distance for the Drone</h3>

      <p>
        The camera mount holding the HM01B0 monochrome camera module is fixed at 45° below the horizontal.
      </p>
      <image-full tag="image" source="./assets/section-3-obj-detection/figure_3_3_camera_mount_angles.png"
      subtitle="Figure 3.3: Camera Mount Angles"></image-full>
      <p>
        Off-board processing inherently introduces latency between image capture, transmission, GCS processing, and command execution. This delay can cause the drone to overshoot the target, landing further away from the Victim Marker than desired, even if still within the 1 m LOS radius. The detection distance is not ideal. As such, tests to optimise the height and speed of the drones was conducted in IDP Studio 1 to decrease the detection distance, However, increasing the speeds and the altitudes lead to a decrease in image resolution due to motion blur, which negatively impacts detection accuracy. As such, a balance must be struck between the height, speed and image resolution of the drones to maintain detection accuracy.
      </p>
      <p>
        The test was conducted by varying the speed of the drone by 0.1 m/s while taking the average of two detection distance readings.
      </p>
      <p>
        The vertical distance between the detected AprilTag and the drone generally decrease as the speed increase (Figure 3.4). However, after increasing the speed beyond 0.5 m/s, there is only a marginal improvement in detection distance. Furthermore, to strike a balance between the drone speed and the turning speed, which is the drone's overall speed scaled down by half when the drone is rotating at a corner, 0.4 m/s was determined as the drone speed.
      </p>
      <p>
        The turning speed of the drone when it is rotating at a corner is 0.2 m/s, which serves as a basis for the Pillar drones’ speed when navigating around the Pillars.
      </p>

      <image-full tag="image" source="./assets/section-3-obj-detection/figure_3_4_graph_distance_vs_speed.png"
      subtitle="Figure 3.4: Graph of Vertical Distance from AprilTag (cm) vs Speed (m/s)"></image-full>
      
      <p>
        The second test was conducted by varying the height (attitude) of the drone by 0.1 m while taking the average of two detection distance readings.
      </p>
      <p>
        The vertical distance between the detected AprilTag and the drone generally increase as the height increase (Figure 3.5), up to a height of 60 cm. The height of the drone was initially decided as 30 cm as the detection distance is the shortest. However, in the RC4 MPSH, the floor surface was very reflective and shiny, which interfered with the velocity estimation of the Flow Deck and the detection accuracy of the camera. The drones had to fly higher to 50 cm to increase the detection accuracy. Beyond 90 cm, the drone will not detect AprilTags.
      </p>
      
      <image-full tag="image" source="./assets/section-3-obj-detection/figure_3_5_graph_distance_vs_height.png"
      subtitle="Figure 3.5: Graph of Vertical Distance from AprilTag (cm) vs Height (m)"></image-full>
      <p>
        To counteract the system latency caused by image transmission, GCS processing amd CPX command, a fixed delay was introduced in the <i>mission_planner.py</i> script after a Victim Marker is detected but before the land command is sent. This allows the drone to fly slightly further, positioning itself more directly over the Victim Marker by the time the land command executes. The test involved repeatedly flying the drones with the delay implemented and then taking the average results to determine the most effective delay time. 1 second delay was chosen. Without delay, the detection distance will be 63.5 cm.
      </p>

      <image-full tag="image" source="./assets/section-3-obj-detection/figure_3_6_landing_distance.png"
      subtitle="Figure 3.6: Detection Distance With Delay Implemented"></image-full>

    </div>

    <div id="sub-section-3-header-5">
      <h3>3.5 Upgrade to Firmware</h3>
      <p>
        Due to issues such as delayed image transmission for a swarm, network robustness issues, and repeated drone disconnections, it was imperative that performance improvements measures be taken.
      </p>
      <p>
        The GAP8 on the AI-Deck was flashed with Wi-Fi Video Streamer example (Bitcraze, n.d.) using a JTAG providing streaming raw images capability from the GAP8 to the GCS. However, streaming RAW files presents several disadvantages: the RAW images is larger than the lossy JPEG image format and needs to be processed by the GAP8, leading to processing time and visible lags. Streaming RAW images from multiple drones simultaneously quickly saturated the Wi-Fi bandwidth, leading to the lag, disconnections, and scalability issues observed by the 2024 team and in our initial multi-drone tests.
      </p>
      <p>
        Figure 3.7 below shows the differences between the original image streaming firmware and the updated lossy JPEG image streaming firmware. The test was conducted in a well-lit room
      </p>

      <image-full tag="image" source="./assets/section-3-obj-detection/figure_3_7_original_vs_updated_image_streaming.png"
      subtitle="Figure 3.7: Original Image Streaming Firmware (Left)  and Updated Image streaming Firmware (Right)"></image-full>

      <p>
        A modified image streaming firmware (Leong, n.d.) that streams Lossy JPEG images and included an autoexposure feature was flashed onto the GAP8. The frame rate is increased to 7 to 8 Frames per seconds (FPS) and the image quality and streaming efficiency improved.
      </p>
      <p>
        This solved our issues marginally. For a swarm of more than twelve stationary drones, there was still delayed image transmission and repeated drones’ disconnections. When a swarm of nineteen drones was tested, only eight drones managed to continuously fly and stream simultaneously at any time. We experimented with reducing the frame rate to ~3.7 FPS to lessen the packet load further. This did not yield significant stability improvements in our tests and potentially risked missing tags during faster movements, so we reverted to the higher ~7-8 FPS rate provided by the JPEG firmware, deciding instead to focus on external network and communication optimization.
      </p>
      <p>
        Further firmware-level optimizations included upgrading the ESP32 Wi-Fi firmware (Bitcraze, n.d.) for potentially better TCP throughput and buffer management, and updating the NRF51 firmware (Bitcraze, n.d.) for improved Crazyradio link startup reliability. These provided marginal gains but did not fully resolve the large-swarm communication bottleneck, necessitating architectural changes.
      </p>
    </div>

    <div id="sub-section-3-header-6">
      <h3>3.6 External System Optimisation</h3>
      <p>
        The SAFMC 2025 strategy required frequent CPX communication for:
      </p>
      <ul>
          <li>Take-off commands</li>
          <li>Landing commands (Victim Markers)</li>
          <li>Ignore commands (implicit for Danger Zones, but requires GCS processing)</li>
          <li>Triggering specific behaviours upon Navigation Aid detection</li>
      </ul>
      <p>
        Due to the heavy reliance on commands sent using the Crazyradio Dongle this year, the Crazyradio throws a lot of errors when a swarm encounters the AprilTags.
      </p>
      <p>
        The SAFMC 2024 team used only three Crazyradio dongles and assigned static radio/channel IDs. This limited architecture struggled even with their simpler mission, as noted in their reports. For SAFMC 2025, the need to potentially send unique commands to multiple drones simultaneously, when several drones detect different Navigation Aids concurrently quickly overloaded the limited Crazyradio. Early swarm tests showed frequent CPX errors and command failures when multiple drones detected AprilTags around the same time, whereas single-drone tests worked well (FIgure 3.8).
      </p>
      <video-component 
      source="https://www.youtube.com/embed/3d6A9wGV5tY" 
      subtitle="Figure 3.8: Video of One Drone Successfully Reversing">
      </video-component>
      <p>
        To solve this issue, our team increased the number of Crazyradio Dongles from 3 to 7. Instead of static assignments, we implemented a dynamic Crazyradio management system within the <i>mission_planner</i>. When a command needs to be sent to a drone, the system checks for an available (idle) Crazyradio dongle. If the primary assigned dongle is busy, it iterates through the pool of available dongles until it finds one free to transmit the CPX packet. This dynamic allocation prevents commands from being dropped due to a single busy radio and is a key improvement over the static SAFMC 2024 approach, enabling more reliable swarm communication under heavy load.
      </p>
      <p>
        Successful swarm scenarios using this enhanced setup showed multiple drones correctly ignoring Danger Zones, landing at Victim Markers, or reacting to Navigation Aids simultaneously or in quick succession, which was not reliably achievable with the previous architecture.(Figures 3.9 and 3.10) 
      </p>

      <video-component 
      source="https://www.youtube.com/embed/TYLihQFfpwo" 
      subtitle="Figure 3.9: Video Swarm Ignoring (Danger Zone)">
  </video-component>

  <video-component 
      source="https://www.youtube.com/embed/RThMP-MyErE" 
      subtitle="Figure 3.10: Video of Swarm Detection (Victim Marker)">
  </video-component>
      <p>
        Initially, SAFMC 2024 team daisy chained two slave routers to a single master router but still faced issues. For SAFMC 2025, we increased the number of GCS laptops from one to three, with one GCS laptops in charge of one area in the Playing Field. Each GCS ran its own instance of the ROS detection nodes and MissionPlanner, communicating with its assigned drones via Wi-Fi for image streaming and its share of the Crazyradio dongles (for CPX commands). While introducing complexity in coordination which were handled via mission pre-planning and drone assignments, this approach offered significant advantages. Each GCS handled image processing and detection for fewer drones around 6 to 7 instead of 19 drones. The Wi-Fi load was distributed across multiple access points and network interfaces. Furthermore, the failure of one GCS would not necessarily bring down the entire swarm.
      </p>
      <p>
        This setup was effective. Even though background noise was a potential factor during all our tests, this setup made it easier track the movement of each drone and the image streaming performance improved drastically. Only one to two drones per GCS laptops facing repeated disconnections during our tests at RC4 MPSH. On SAFMC 2025 competition day, all nineteen drones streamed images to their GCS successfully without disconnections or freezing.
      </p>

      <image-full tag="image" source="./assets/section-3-obj-detection/figure_3_11_router.png"
      subtitle="Figure 3.11: Old Router Setup and Modified Router Setup"></image-full>

    </div>

    </div>
    

    </br></br>
      
    <div id="section-header-4" class="section" style="display: none;">
      <h2>4. Search Strategy</h2>
      <image-component tag="image" source="./assets/section-4-mission-strategy/figure_4_1_comparison_search.png"
      subtitle="Figure 4.1: A Comparison between Mapped and Mapless Search Strategy"></image-component>

      <div id="sub-section-4-header-1">
          <h3>4.1 Search Strategy</h3>
  
        

      </div>

      <div id="sub-section-4-header-2">
          <h3>4.2 Object Detection</h3>

      </div>

      <div id="sub-section-4-header-3">
          <h3>4.3 Navigation Aids</h3>

      </div>

</div>
    <br />
    <br />

      <div id="section-header-5" class="section" style="display: none;">
          <h2>5. Known Search Area (Yan Yew)</h2>
            <p>
              The Known Search Area in the Playing Field presents a structured environment where positions of static obstacles like Inner Walls are known. This information can be leveraged to preplan drone paths, ensuring complete coverage of the Known Search Area. This knowledge of the layout complements well with a mapless search strategy. 
            </p>
            <p>
              Compared to a mapped approach, which relies on a complete map or Simultaneous Localization and Mapping (SLAM), a mapless approach is significantly less computationally demanding and simpler to implement. A mapped strategy requires substantial processing power for SLAM and adds complexity, as the maps generated by each drone in the swarm must be merged and transmitted back to the drones, often via a GCS, which introduces a single point of failure. While normally mapless strategy does not inherently guarantee full coverage, since in this case the obstacle layout in the Known Search Area is known, optimization of search strategy to maximize coverage is possible hence a mapless approach would be a better option.
            </p>
            <p>
              To ensure a robust and scalable strategy, a decentralized strategy is adopted, where each drone operates independently without relying on centralized communication. This is achieved by writing the writing the navigation and decision-making code in C and flashing it directly onto the Crazyflie firmware.
            </p>
            <p>
              This section details the implementation of decentralized Modified Swarm Bug Algorithm (MSBA). Additionally, this section discusses the use of simulation in optimizing the search algorithm parameters, and key features like multi-airways for collision avoidance, and reverse commands to define search boundaries.
            </p>
      <div id="sub-section-5-header-1">
        <h3>5.1 Selection of Search Algorithms</h3>
          
          <p>
            The decentralized mapless search algorithms considered were Parallel Sweep Search, MSBA, and Pre-planned Flight Paths. These algorithms were shortlisted because they do not require a map, rely on heuristic that can easily be implemented onboard of the drones to form a decentralized system, and can be implemented in a multi-agent system like a drone swarm.
          </p>
          <p>
            Parallel Sweep Search is a search strategy that is commonly used in the maritime industry in search and rescue mission. Using multiple ship vessels to search a large area when location of search target is unknown. The search algorithm involves making the drones move in parallel, evenly spaced lanes that are offset to one another. When encounter a wall, the drone will perform wall-following around the obstacle until it returns to its original planned path then resume the sweep search. 
          </p>

          <image-component tag="image" source="./assets/section-5-known-area/figure_5_1_parallel_sweep_search.png"
          subtitle="Figure 5.1: Desired Paths of Two Drones Performing Parallel Sweep Search"></image-component>

          <p>
            The MSBA is an adaptation of the Swarm Gradient Bug Algorithm (SGBA) developed by TU Delft. This algorithm follows an identical heuristic as bug algorithm where the drone would travel in a specific heading, then encountered an obstacle it performs wall-follow around it until its original heading is free of obstacle. This algorithm is explained in detail in Section 5.2. 
          </p>
          <p>
            The pre-planned flight path strategy involves programming a series of predefined movements for each drone before deployment, which is possible because of the environment being known. For example, as illustrated in Figure 5.2, if we want the drone to reach point X, the drone will be programmed to fly forward. When obstacle is detected by the front ToF sensor, the drone should turn right and continue forward until it detects another obstacle using the front ToF sensor.
          </p>

          <image-component tag="image" source="./assets/section-5-known-area/figure_5_2_pre_planned_flight_path.png"
          subtitle="Figure 5.2: Pre-planned flight path strategy: Diagram illustrating the drone’s movement from the start area to point X. "></image-component>
          <p>
            The decentralized mapless strategies shortlisted above are then evaluated using a weighted decision matrix. The main factors to consider were (1) search coverage, (2) adaptability, (3) exploration speed, and (4) implementation complexity. For each of these strategies, the search strategies are given a score out of 3 for how well it performs.
          </p>
          <p>
            Table 4.0: Evaluation of Parallel Sweep Search, Modified Swarm Bug Algorithm, and Pre-planned Flight Path Search Strategies
          </p>
          <image-component tag="image" source="./assets/section-5-known-area/table_4_decision_matrix_search.png" subtitle=""></image-component>
          <p>
            Search coverage evaluates the ability of the algorithm to cover the entire search area. A higher coverage indicates a more effective exploration to search for victim markers in the Playing Field. This criterion is assigned the highest weight because the competition’s scoring criteria is based on the number of victim markers rescued. A higher coverage increases the likelihood for the swarm drone to locate these victim markers placed throughout the Playing Field. Parallel Sweep Search is given the highest score because theoretically with multiple drones offset to each other, the swarm will be able to cover the entire search Playing Field. MSBA and pre-planned flight path both can provide high search coverage but due to the heuristic, they won’t perform better than Parallel Sweep Search.
          </p>
          <p>
            Reliability concerns with the predictability, repeatability, and robustness of the strategies. High reliability is important because the competition only allows each team to perform two runs, so a reliable solution should be used to ensure consistent performance during the competition. Parallel Sweep Search is given the lowest score because it depends heavily on localization and precise pathing. Based on a drift test conducted on the flow deck, the uncertainty in the localization the optical flow sensor is ±10 %, which is not precise. MSBA is given the highest score because it depends on heuristic that utilizes the ToF distance sensors, which are highly precise. The pre-planned flight path is given a similar high score of 3 for a similar reason.
          </p>
          <p>
            Adaptability is defined as the ability for the algorithm to maintain consistent search coverage performance in environments with different obstacle sizes. Although the layout of the obstacles in the Known Search Area is known, the dimensions of these obstacles are not. An algorithm that is highly adaptable should be able to maintain consistent behaviour, ensuring reliable search coverage across various obstacle configurations. MSBA is the most adaptable because it allows the drones to react to obstacles and adjust their behaviour accordingly. Parallel Sweep Search can handle obstacle with wall-following behaviour. However, it can be prone to failure if the dimensions of certain walls exceed the range considered during the planning phase. The pre-planned flight path has the lowest adaptability because the flight paths must be tailored to specific Playing Field configurations.
          </p>
          <p>
            Exploration speed is the amount of time it would require for the swarm drone to search a fixed area. This criterion is given the lowest weight because the runtime of the drones is not a scoring criterion but instead used as a tiebreaker. The Parallel Sweep Search is the slowest because of the sweep pattern that drones need to trace. MSBA and the pre-planned strategies are given the highest score of 3 because the drones follow paths that generally lead from the start area to the end of the Playing Field without the need for backtracking.
          </p>
          <p>
            Based on decision matrix evaluation above, MSBA stands out as the most well-rounded search strategy. MSBA excels in reliability, adaptability, and exploration speed, making it the most suitable approach for the swarm drone search mission. Its ability to respond to obstacles dynamically and adjust its behaviour accordingly allows it to maintain consistent performance across different environments, particularly in the Known Search Area with its unpredictable obstacle dimensions.
          </p>
        </div>

      <div id="sub-section-5-header-2">
      <h3>5.2 Modified Swarm Bug Algorithm</h3>
          <p>
            MSBA is a heuristic algorithm governed by a few simple rule-based behaviour as illustrated in Figure 5.3. After take-off, the drone rotates to its preferred heading and then moves forward in that direction. Upon detecting an obstacle, it initiates wall-following until its preferred heading is clear of obstacles.
          </p>
          <p>
            To prevent the Crazyflie from getting trapped in an indefinite loop, a loop detection mechanism is implemented. This mechanism modifies the preferred heading when the drone is detected to be stuck. When the Crazyflie encounters an obstacle and begins wall-following, it records the hit point using odometry data from the Flow Deck. If the drone moves toward the same hit point again, the loop detection mechanism is triggered, adjusting the navigation path to break the loop. 
          </p>
          <image-component tag="image" source="./assets/section-5-known-area/figure_5_3_MSBA.png"
          subtitle="Figure 5.3: Visualization of MSBA"></image-component>
          <p>
            The performance of MSBA is determined by the coverage, speed, and reliability of the swarm drones. Ideally, the swarm would cover all the Known Search Area as fast as possible, and reliably by having reproducible results and zero collisions among the drones. The performance greatly relies on key parameters such as flight speed, altitude, wall-following direction, corresponding wall-following distance. Wall-following direction refers to the side of the wall that the drones use as a guide while navigating. Wall-following direction refers to the relative position of the wall that a drone maintains while navigating. A left wall-following direction means the drone keeps the wall to its left as it moves, while a right wall-following direction means the drone keeps the wall to its right.
          </p>
      </div>

      <div id="sub-section-5-header-3">
        <h3>5.3 Initial Proposal Strategy Using MSBA</h3>
          <p>
            The team has allocated a total of 10 drones to search the Known Search Area. To obtain the optimal parameters, the ideal paths of the 10 drones to maximize coverage are first planned in accordance with the heuristic of the algorithm. Then the parameters required to achieve the corresponding flight paths are then determined by working backwards. 
          </p>          
          <p>
            The strategy to perform search in the Known Search Area is illustrated in Figure 5.4. It involves deploying the drones on the left side of the Playing Field, ensuring they reach designated dispersion points—Point A, Point B, and Point C, which the drones will then fan out in their respective preferred headings. Each dispersion point is assigned three drones. To guide the drones to their respective dispersion points, they will follow a wall-following manoeuvre around Wall 1, Wall 2, and Wall 3 before dispersing into the search area. 
          </p>
          <p>
            The flight altitude of all drones is set to 0.3 m because real-life test flight at that height provided great flight stability. The speed of drones is set to 0.4 m/s as it is the optimal speed for stable flight performance after performing real-life testing which will be explained in later section in Section 5.6. This speed also ensures reliable AprilTag detection as tested by the Object Detection system (Section 3.4).
          </p>
        <image-component tag="image" source="./assets/section-5-known-area/figure_5_4_maximise_coverage.png"
          subtitle="Figure 5.4: Strategy to maximise search coverage using 9 drones running MSBA"></image-component>
          <p>
            For intra-drone collision avoidance strategy to prevent drone-to-drone collisions, a two-lane method was implemented. Depending on the direction of wall-following, the drones would maintain different distance to walls to create two separate lanes. Extensive real-world testing a small-scale Playing Field, along with simulations, confirmed the reliability of this strategy, provided that the drones are correctly positioned at the starting area The optimal wall distances were determined to be 0.6 m for drones performing left wall-following and 1.2 m for those performing right wall-following, as illustrated in Figure 5.5 below.
          </p>
          <p>
            Additionally, the loop detection transformation angle was set to be 90 ° instead of the 180 ° proposed in the original SGBA to prevent drones from travelling straight back to the starting area. The transformation angles depend on the drones' wall-following direction and follow the right-hand rule convention: drones assigned to right wall-following adjust their preferred heading by +90 °, while those following the left wall adjust by -90 °.
          </p>
          <p>
            Since all the drones must take off within two waves, to ensure the drones reach their respective dispersion points safely without collision, drones belong to Point B will first take off, followed by drones that travel to Point A and Point C. This configuration will also prevent drone collisions by ensuring Point B drones to be in the forefront, followed by Point C and Point A drones that will be unlikely to collide with one another as they travel in opposite directions.
          </p>
          <p>
            The drone configurations in the Start Area to guide the 3 separate drone groups to dispersion points A, B, C is shown below in Figure 5.5. Each drone is also assigned wall-following direction of either left or right. Drone 1, 2, 3 will first take off and travel to Point A, then followed by Drone 4, 5, 6 to Point C and Drone 7, 8, 9 to Point A. The drones in each group are arranged in a "V" formation. Reason being because based on real-life swarm drone testing, maintaining a clear LOS in the forward direction is crucial, especially during take-off. If a drone detects another drone directly in front of it, it will automatically initiate wall-following mode, potentially disrupting the planned dispersion sequence. 
          </p>
          <image-component tag="image" source="./assets/section-5-known-area/figure_5_5_initial_configuration.png"
          subtitle="Figure 5.5: Initial configurations of drones in the start area"></image-component>
      </div>

      <div id="sub-section-5-header-4">
        <h3>5.4 Simulation to Evaluate Search Strategy</h3>
          <p>
            To validate, evaluate and further optimize the above proposed strategy, extensive testing is required. Performing small scale tests by recreating subsections of the Playing Field is not possible because it is difficult to predict the state of the drones at the boundaries of the subsections. Full-scale is also not feasible because of the space constraints at Studio 1. Therefore, a simulation environment was created. 
          </p>
          <p>
            Beyond addressing space limitations, simulation enables rapid troubleshooting of search algorithms and fine-tuning of parameters without the need to physically redeploy the code onto the drones every time a change is made. Webots was used as simulator because it allows individual robot in the environment to execute is own C code, which mirrors the actual implementation of the decentralized search strategy using MSBA. A key consideration in developing this simulation environment was ensuring that the same code deployed onto the drone firmware could be tested within the simulation.
          </p>
          <p>
            To create a decentralized system, the MSBA scripts are all running onboard of Crazyflie. The scripts are all compiled on the development workspace before flashing onto the drone using Crazyradio. This will write the scripts within the app layer in the Crazyflie firmware. When the drone starts up, the firmware will execute the scripts inside the app layer, which then have access to all the deck sensor APIs, enabling the drones to operate autonomously without the need for centralized control.
          </p>

          <image-component tag="image" source="./assets/section-5-known-area/figure_5_6_firmware_onboard.png"
          subtitle="Figure 5.6: Architecture diagram of firmware on board of real Crazyflie"></image-component>
          <p>
            While Webots provides an excellent platform for simulating robots running onboard scripts, it does not support the Crazyflie firmware, or the identical Deck API used in the actual Crazyflie hardware. As a result, to integrate MSBA with the Webots simulation, an interface is required to bridge the gap between MSBA and the simulation environment. This interface allows the MSBA to access the necessary sensor readings, such as distance data from virtual sensors, and provides a means for the Webots robot to receive output from the MSBA, ensuring that the drone's behaviour in the simulation mirrors that of the actual Crazyflie system.
          </p>
          <p>
            The full Webots repository that contains the simulation environment and controller code can be found here https://github.com/CDE-4301-ASI-401/Webots_CF2.
          </p>
          <image-component tag="image" source="./assets/section-5-known-area/figure_5_7_firmware_webots.png"
          subtitle="Figure 5.7: Architecture diagram of simulated Crazyflie in Webots"></image-component>
          <p>
            The competition search environment was first recreated in Webots as shown in Figure 5.8, with the static obstacles being placed according to the Playing Field layout illustrated in the competition challenge booklet while conforming to the minimum distance. Each box represents a 1 m by 1 m area. <i>Note that the orientation of the top-down view differs from the previous figures. The view in this figure is rotated clockwise by 90 °</i>.
          </p>
          <image-component tag="image" source="./assets/section-5-known-area/figure_5_8_webots_simulation_environment.png"
          subtitle="Figure 5.8: Webots Simulation Environment"></image-component>
          
          <p>
            The performance of the search strategy was then evaluated by simulating the drones in different Playing Field configurations. The drone paths after 4 minutes of simulation are plotted and compared using the drones position data stored in csv files and Matplotlib. The percentage of search coverage is also determined by counting the box which the swarm drone’s paths crossed in the Known Search Area overlayed in Figure 5.5. 
          </p>
          <p>
            In all the configurations tested, the drone swarm was able to cover most of the Known Search Area collectively, consistently achieving a search coverage above 80%. However, there are a few blind spots that would be miss by the drones. Notably, the area in corners. Figure 5.9 shows the results from 3 of the simulations performed, as well as the blind spots. In addition, in certain scenarios, some drones will travel into the Pillar Area, which is undesired.
          </p>
          <p>
            After multiple simulations, the two-lanes collision avoidance strategy proved reliable, as no collisions occurred beyond the initial take-off phase where drones fly to their respective dispersion points. Drone-to-drone collisions were only observed between drones in the same group, primarily due to insufficient spacing between drones in the Start Area.
          </p>
          <image-component tag="image" source="./assets/section-5-known-area/figure_5_9_Webots_Simulation.png"
          subtitle="Figure 5.9: Simulated paths of drones performing MSBA in a full-scale arena after 4 minutes"></image-component>

          
      </div>

      <div id="sub-section-5-header-5">
        <h3>5.5 Improvements to MSBA</h3>
          <p>
            To address blind spots in the corners of walls, two wall-following (WF) drones will be deployed to cover these areas, as identified in the simulation. To prevent these WF drones from interfering with the paths of the MSBA drones, these WF drones will be fly at a higher altitude of 0.55 m, 0.25 m higher than the altitude of MSBA drones. At this altitude, the effect of downwash on the MSBA drones flying below the WF drones is minimal and has no significant impact on image detection as tested (Section 3.4). 
          </p>
          <p>
            A total of ten drones are assigned to search the Known Search Area. Since only nine drones were initially used to test the MSBA search strategy, an additional tenth drone will be introduced to perform WF along the outer perimeter of the Playing Field, while drone number nine will be reassigned to WF as well. These drones will maintain a wall-following distance of 0.5 m to handle cases where object markers are placed in the corners. With the addition of a new drone and the reassignment of one drone to WF, the search coverage increased to over 90 %, calculated using the same method described previously.
          </p>
          <p>
            To address the scenario where drones cross into the Pillar Area like the fourth figure in Figure 5.9, a reverse function was implemented in MSBA to enable the drones to perform 180 ° turn and continue searching within the Known Search Area. To facilitate this mechanism, two April Tag Navigation Aids will be placed at the border between Known Search Area and Pillar Area. When an MSBA drone in the Known Search Area detects them, the Mission Planner will broadcast a reverse command packet to the detecting drone. Upon receiving this packet, the drone will adjust its preferred heading by 180 °, ensuring it remains within the designated search area.
          </p>
          <image-component tag="image" source="./assets/section-5-known-area/figure_5_10.png"
          subtitle="Figure 5.10: Placements of two Navigation Aids at the border between Known Search Area and Pillar Area"></image-component>
      </div>

      <div id="sub-section-5-header-6">
        <h3>5.6 Real Life Testing</h3>
          <p>
            To validate the MSBA implementation and further optimize flight parameters, a series of real-world tests were conducted in three different environments, where 1 m by 1 m cardboards are used to create wall obstacles:
          </p>
          <ol>
            <li>Small-scale tests in the studio</li>
            <li>Medium-scale tests in a seminar room at E7 </li>
            <li>Full-scale tests in RC4 Hall  </li>
          </ol>
          <p>
            Initially, most tests were small-scale tests conducted in E2A Studio 1 to optimize individual drone flight stability by tuning flight parameters such as wall-following distance, flight speed, and altitude. Prior to creating the simulation environment, these small-scale tests provided the optimum flight parameters for the MSBA drones which was later used in the simulation.  The small-scale test also enabled the reverse function to be tested by manually issuing the reverse command packet before integrating it with the Mission Planner system.
          </p>
          <image-component tag="image" source="./assets/section-5-known-area/figure_5_11_small_scale_test.png"
          subtitle="Figure 5.11: Small-scale test environment in E2A Studio 1"></image-component>
          <p>
            A key issue observed when testing the WF drones was that they had difficulty turning around thin wall edges of the L-shaped free-floating wall in the Known Search Area. When turning around these edges, the drones would lose track of the wall edges due to the narrow field of view of the side ToF sensor. To mitigate this, 2 wall guards are used as Navigation Aids. These Navigation Aids are created by creating a trifold using a corrugated board and are used by slotting them around the wall edges of the L-shaped wall. By effectively increasing the wall-thickness around the wall edges, the WF performance is significantly improved.
          </p>
          <image-component tag="image" source="./assets/section-5-known-area/figure_5_12_wall_guard.png"
          subtitle="Figure 5.12: Wall-guard navigation aid used to increase thickness of wall edges"></image-component>
          <p>
            Subsequently, medium-scale tests were performed in E7 seminar rooms. With a larger space, subsections of the Known Search Area were recreated to test the behaviour of swarm clusters (around 3 to 5 drones). These tests validated the Webots simulation, as the drones’ behaviour and paths closely matched those generated in the simulation. However, they also revealed challenges unique to executing the MSBA search strategy in the real-world.
          </p>
          <p>
            The MSBA drones’ paths were highly dependent on their initial configuration in the Start Area. During the first few tests, the MSBA drones often deviated from the intended path due to inaccurate initial orientation when placed at the Start Area. In some cases, this resulted in the drones failing to reach reaching the dispersion zone in the Known Search Area, as they miss the walls that would have guided them correctly. Additionally, when drones were placed too close to one another, they either exhibited erratic after detecting the drone in front as a wall obstacle or, in worse the worst case, collided with the drones ahead.
          </p>
          <p>
            Unlike in simulation, where drone placements are accurate and precise, the initial medium-scale tests revealed the significant impact of human error on overall search coverage. To mitigate deviations in the initial preferred heading of MSBA drones, laser pointers were used to align them correctly in the Start Area. Furthermore, to prevent drones from being too close to one another, they were spaced farther apart to fully utilize the large Start Area.
          </p>
          <p>
            Finally, full-scale tests were conducted in RC4. The 20 m-by-20 m competition Playing Field was recreated to evaluate the performance of the entire swarm drone Initially, the drones exhibited poor localization, drifting, sudden accelerations, and jerky movements due to the venue’s reflective flooring. This issue is discussed in detail in Section 9. To mitigate this problem, all drones were flown at a higher altitude—MSBA drones were raised from 0.3 m to 0.5 m, while WF drones were increased from 0.55 m to 0.75 m. During these tests, the drones closely followed the same paths as their simulated counterparts in Webots.
          </p>
          <image-component tag="image" source="./assets/section-5-known-area/figure_5_13_full-scale_test_Known_Area.png"
          subtitle="Figure 5.13: Full-scale testing of Known Search Area drones "></image-component>
      </div>

      <div id="sub-section-5-header-7">
        <h3>5.7 Final MSBA Configurations</h3>
        <p>
          After conducting the series of real-world tests, the set of flight parameters for Known Search Area drones performing MSBA and wall-following were determined. 
        </p>
        <p>
          Table 5.0. Key parameters of drones performing MSBA and WF in the Search Area during actual competition
        </p>
        <image-component tag="image" source="./assets/section-5-known-area/table_5_known_config.png" subtitle=""></image-component>
        <br><br> 
        <image-component tag="image" source="./assets/section-5-known-area/figure_5_14_final_known_start_area_config.png"
          subtitle="Figure 5.14. Drone position and orientation at the Start Area to execute the MSBA search strategy"></image-component>
        <p>
          During the actual competition, three regular victims were placed within the Known Search Area for rescue. It should be noted that the placement of Victim and Danger Markers varied between Run 1 and Run 2. The take-off sequence and navigation to the dispersion points worked as intended. However, further drone behaviour could not be directly observed as the competition Playing Field was off-limits during the mission runs. The final results for the Known Search Area are outlined in Table 6.0.
        </p>
        <p>
          Table 6.0. Number of victims rescued in the Known Search Area
        </p>
        <image-component tag="image" source="./assets/section-5-known-area/table_6_Known_area_rescues.png" subtitle=""></image-component>
 
        <p>
          In the first run, two out of three regular victims were successfully rescued. However, before the run began, the wall-guard Navigation Aid, which was designed to increase the thickness of the wall edges, was mistakenly left out and not placed around the L-shaped wall in the Known Search Area. This issue was rectified before the second run, and during this run, all of the regular victims were rescued. However, since the victim placements differed between runs, it was not conclusive whether the victim missed in Run 1 was located near the L-shaped wall. Without the Navigation Aid in place, the wall-following drone was unable to reach it.
        </p>
      </div>

    </div>

      <br />
      <br />

      <div id="section-header-6" class="section" style="display: none;">
        <h2>6. Unknown Search Area (Yan Yew)</h2>
          <p>
            The Unknown Search Area is an 8 m by 8 m area situated in the centre of the Playing Field accessible through two entrances/exits to the right and at the top of the area. Unlike the Known Search Area and Pillar Areas that are easily accessible after crossing the Start Area, drones searching the Unknown Search Area need to first reach either entrances to the Unknown Search Area, then start searching the areas. 
          </p>
          <p>
            A mapped approach was initially explored to ensure full coverage of the Unknown Search Area. This would require drones to perform SLAM to generate occupancy grids of the environment, enabling the use of search strategies such as frontier-based exploration, where all drones assigned to the Unknown Search Area would simultaneously map and search, or path planning algorithms, where a subset of drones focuses on mapping and marking victim locations while the rest are dedicated to rescue operations. In this approach, once victims are detected, a path planning algorithm would determine the optimal route and direct the rescue drones accordingly.
          </p>
          <p>
            This section documents the feasibility tests conducted on the mapped approach and the challenges encountered, ultimately leading to the adoption of MSBA for searching the Unknown Search Area. Initial experiments involved performing SLAM using the multi-ranger deck onboard the Crazyflie to generate occupancy grids. However, due to sensor range limitation and poor localization, SLAM proved unreliable for real-time mapping and navigation. These constraints made frontier-based exploration and path planning strategies impractical. Given these limitations, the MSBA originally designed for the Known Search Area was adapted for the Unknown Search Area drones as well. 
          </p>
  
        <div id="sub-section-6-header-1">
          <h3>6.1  Feasibility Testing of Mapping Hardware</h3>
            <p>
              A centralized approach using ROS2 was chosen to perform SLAM due to the limited computation resources available on the STM onboard the Crazyflie. This approach provided scalability and ease of implementation, as it can be easily integrated with the Mission Planner, which also operates on ROS2. This approach would also allow us to leverage on existing tools and libraries for SLAM. To implement SLAM on a swarm of drones, each individual drone needs to produce an accurate map. These individual occupancy maps can then be merged to create a single, complete map.
            </p>
            <p>
              The performance of SLAM using a single Crazyflie was first tested using the Crazyflie ROS2 Multiranger package. This multi-ranger package uses the Crazyswarm2 package to collect sensor data from the Crazyflie via the Crazyradio, which is then used to construct an occupancy map. 
            </p>
            <p>
              For testing, a 4m by 5m enclosed space was set up in the E2A Studio. A single Crazyflie was deployed to perform SLAM while following the walls within the enclosed space. Upon completing a full loop and returning to its starting position, the drone was terminated, and the resulting map was evaluated.
            </p>
          <image-component tag="image" source="./assets/section-6-unknown-area/figure_6_1_enclosed_space.png"
          subtitle="Figure 6.1: Enclosed space (4m by 5m) set up to test Crazyflie SLAM performance"></image-component>
          <p>
            During the initial tests, the maps produced by the drone was poor that did not remotely match the ground truth. In one of the runs, it was observed that when the drone performed SLAM, it was leaving an “occupied” trail in Figure 6.2. Upon inspection, it was discovered that the ToF sensors on the multi-ranger deck can be obstructed by the AI deck located below it, as shown in Figure 6.2. After lifting the multi-ranger up to ensure the ToF sensors are unobstructed and cleaning the ToF sensors, this single Crazyflie was able to consistently produce accurate map.
          </p>
          <image-component tag="image" source="./assets/section-6-unknown-area/figure_6_2_occupacy_map.png"
          subtitle="Figure 6.2: Occupancy map produced by single Crazyflie when its back TOF sensor was obstructed"></image-component>
          <br><br>
          <image-component tag="image" source="./assets/section-6-unknown-area/figure_6_3_obstructed_tof.png"
          subtitle="Figure 6.3: Obstructed TOF sensor on Crazyflie’s Mult-Ranger deck"></image-component>
          <br><br>
          <image-component tag="image" source="./assets/section-6-unknown-area/figure_6_4_SLAM_Single_Crazyflie.png"
          subtitle="Figure 6.4: Occupancy map produced by Crazyflie performing wall-following"></image-component>
          <p>
            With a single Crazyflie producing a reliable SLAM map, the next step was to evaluate multi-drone SLAM by merging occupancy maps from multiple Crazyflies. To achieve this, the ROS2 Merge Map package was integrated into the system. This ROS2 node subscribes to multiple map topics and publishes a single merged occupancy map, allowing the GCS to construct a complete map of the environment. The final implementation can be found in Github repository.
          </p>
          <p>
            To test the multi-drone SLAM and map merging implementation, two Crazyflies were deployed in a 4 m by 4m enclosed space, performing SLAM while wall-following in opposite directions. Initially, after both drones completed their first loop, the merged map was accurate and closely matched the actual environment. However, as the drones continued to complete additional loops, the mapped environment began to drift over time, leading to misalignment in the final merged map (Figure 6.X). This issue was attributed to drift in the individual Crazyflie’s optical flow sensor readings. As the drones continued wall-following around the enclosed space, positional errors accumulated significantly, causing increasing discrepancies between the mapped and actual environments.
          </p>
          <image-component tag="image" source="./assets/section-6-unknown-area/figure_6_5_Two_Crazyflie_SLAM_environment.png"
          subtitle="Figure 6.5:Test environment to evaluate the performance of Crazyflie SLAM using onboard multi-ranger deck"></image-component>
          <br><br>
          <image-component tag="image" source="./assets/section-6-unknown-area/figure_6_6_merge_map.png"
          subtitle="Figure 6.6: Merge map generated by two drones performing wall-following around enclosed space in Figure 6.5 "></image-component>
          <p>
            To mitigate map drift in multi-drone SLAM, proper loop closure detection must be implemented. Loop closure detection works by extracting features from the current sensor data and comparing them with previous sensor data. If a match is found, a loop closure constraint is placed in the SLAM map, reducing cumulative error (Nashed, 2020). 
          </p>
          <p>
            However, implementing loop closure detection requires a large amount of data to overcome inherent sensor noise. This is not feasible with the ToF sensors on the Crazyflie due to their narrow field of view. Sparse sensing is a niche research area in SLAM that aims to improve performance using limited sensor data. One study suggests leveraging fast convex optimization techniques to implement loop closure detection (Latif et al., 2017). However, without any publication on actual implementation, integration such method to Crazyflie will require too much time and resources. 
          </p>
          <p>
            ETH-PBL recently developed Nano Swarm Mapping, a decentralized solution tailors for nano drones like the Crazyflie. Unlike a centralized SLAM solution which relies on a GCS to process sensor data and construct maps, this solution enables each drone to independently perform scanning of environment onboard using ToF sensors, then broadcasting the scans and poses to the “main drone”, a drone within the swarm that is assigned to collect the scan data, perform SLAM finally broadcasting the results back to the swarm. To mitigate the effect of drifts, the researchers designed a modified multi-ranger deck which features ToF sensors with wider FOV and was able to implement loop closure detection (Friess et al., 2024). Despite its advantages, adopting this solution was not feasible due to the requirement of modified multi-ranger that was not readily accessible.
          </p>
        </div>
  
        <div id="sub-section-6-header-2">
          <h3>6.2 Feasibility Testing of Mapping Software</h3>
        
        </div>


        <div id="sub-section-6-header-3">
          <p>
            Due to the poor performance of SLAM using multiple drones, it was decided to implement MSBA for the Unknown Search Area drones to ensure sufficient tests can be conducted before the competition. Six drones are assigned to the Unknown Search Area (drone number 4, 5, 6, 7, 8, 9), the three even-numbered drones (4, 6, and 8) will enter from the top opening of the Unknown Search Area while the remaining odd-numbered drones (5, 7, and 9) will enter from the right opening, this will help maximising search coverage as the drones will be more spread out, and to tackle the possibility of the Unknown Search Area consisting two subsections that are only accessible through the two separate openings.
          </p>
          <div id="sub-section-6-header-3-header-1">
            <h4>6.3.1 Drone Guidance to the Unknown Search Area</h4>
            <p>
              To get to the openings to the right and top of the Unknown Search Area, the drone will travel through the passageway between the Unknown Search Area right outer wall and the pillars. The right opening will be referred to as Entrance 1, and the top opening will be referred to as Entrance 2. The 3 drones that will enter from Entrance 1 belong to Group 1 and the other 3 drones that enter from Entrance 2 belong to Group 2. 
            </p>
            <image-component tag="image" source="./assets/section-6-unknown-area/figure_6_6_desired_paths.png"
            subtitle="Figure 6.7: Desired paths of Group 1 and Group 2 drones to navigate from Start Area to the Unknown Search Area"></image-component>
            <p>
              The desired paths of Group 1 and Group 2 drones are first considered from Start Area to the respecting Unknown Search Area openings as illustrated in Figure 6.1 above. There are 3 waypoints along the path from the Start Area to Entrance 2 (orange arrow in Figure 6.1) where key events take place. When a drone reaches point A, it should continue moving forward if it belongs to 2, otherwise turn left into Entrance 1 and begin searching the Unknown Search Area. At point B, the drones should turn left and continue moving forward. At point C, the drones should turn left into the Unknown Search Area and start searching for victim markers.
            </p>
            <p>
              To achieve the desired paths, Navigation Aids will be placed on point A, B, and B, which will be referred to Tag A, Tag B, and Tag C respectively. Whenever these tags are seen by the drones, a radio command unique to the Navigation Aid will be issued to that drone. At the Start Area, all the Unknown Search Area drones will be lined up with point A and B being straight forward to them. 
            </p>
            <p>
              According to Figure 6.7, there are three unique possible motions that the drones would perform to enter the Unknown Search Area, namely (1) Continue straight forward, (2) Turn left, then continue straight forward, and (3) Turn left, move forward, and begin search. The pseudocode is shown in Algorithm 1, which is implemented in SGBA.c. 
            </p>
            <image-component tag="image" source="./assets/section-6-unknown-area/Algorithm_1.png"
            subtitle=""></image-component>
            <p>
              Since in Mission Planner there is only an only a one-to-one mapping of radio command to Navigation Aid. The logic to initiate the correct motion when received a radio command needs to be handled by the drone. For example, when a Group 1 drone received a radio command that corresponds to Tag A, it needs to initiate motion (3) Turn left, move forward, and begin search. Whereas a Group 2 drone receiving the same radio command should initiate motion (1) Continue straight forward. The pseudocode is included in Algorithm 2, which is implemented in state_machine.c.
            </p>
            <image-component tag="image" source="./assets/section-6-unknown-area/Algorithm_2.png"
            subtitle=""></image-component>
          </div>

          <div id="sub-section-6-header-2-header-2">
            <h4>6.3.2 MSBA for Searching the Unknown Search Area</h4>
              <p>
                Like the MSBA drones in the Known Search Area, these six Unknown Search Area drones have the same set of parameters to be optimized. Without knowledge of the obstacle layout in the area, the six drones will be evenly distributed to maximize search coverage, as shown in Figure 6.8. The preferred angles for each drone are relative to the forward direction, using the right-hand convention.
              </p>
              <image-component tag="image" source="./assets/section-6-unknown-area/figure_6_8_unknown_preferred_directions.png"
            subtitle="Figure 6.8: Preferred direction of MSBA drones after entering Unknown Search Area"></image-component>
              <p>
                To ensure the drones remain within the Unknown Search Area, Navigation Aids placed at the Unknown Search Area openings will be used to guide them. When a drone detects these Navigation Aids, the GCS will issue the corresponding radio command packet to the drone. Once the drones have entered the Unknown Search Area and are on a path to exit, they will again detect these Navigation Aids. Upon receiving the radio command packet from the GCS, the drones' preferred direction will be adjusted to keep them within the boundaries of the Unknown Search Area. The optimal position of Navigation Aids after extensive real-world testing are shown in Figure 6.8.
              </p>
              <image-component tag="image" source="./assets/section-6-unknown-area/Algorithm_3.png"
            subtitle=""></image-component>
            <p>
              Real-world testing with various transformation values revealed that a 135 ° transformation is the most optimal. While changing the preferred heading by 180 ° would be the most effective way to keep the drones within the Unknown Search Area, it would not improve search coverage, as the drone would simply retrace its path in the opposite direction. A 90 ° transformation is not used, as it can cause the drone to exit the Unknown Search Area when approached at certain angles.
            </p>
            <p>
              The Unknown Search Area drones will adapt a similar intra-drone collision avoidance strategy as the Known Search Area drones, by flying the two groups of drones at different distance to wall and altitude. Group 1 drones will fly at 0.75 m, while group 2 drones will fly at 0.5 m. Due to the absence of wall-following drones covering area in wall corners, the drones will fly slightly closer to the wall compared to the MSBA drones in the Known Search Area. Group 1 drones (5, 7, 9) will perform left wall-following 0.4 m from the wall, group 2 drones (4, 6, 8) will perform right wall-following 1.0 m from the wall. All the drones will fly with the same maximum speed of 0.4 m/s.
            </p>
            <p>
              Table 7.0. MSBA parameters of Unknown Search Area drones
            </p>
            <image-component tag="image" source="./assets/section-6-unknown-area/table_7_unknown_final_msba_parameters.png"
            subtitle=""></image-component>
            <p>
              The Unknown Search Area MSBA drones will be lined up in the Start Area in the order of 4, 6, 8, 5, 7, 9. To mitigate the risk of collisions during take-off, the drones will be spaced equally apart, with both groups of drones taking off in separate wave. Group 2 drones (4, 6 ,8) will take off in the first wave to enter the top opening of Unknown Search Area, whilst the group 1 drones (5, 7, 9) will take off in the second wave, entering the Unknown Search Area from the right opening. 
            </p>
            <image-component tag="image" source="./assets/section-6-unknown-area/figure_6_9_unknown_start_config.png"
            subtitle="Figure 6.9: Configurations of Unknown Search Area drones in Start Area"></image-component>
          </div>
        </div>
  

        <div id="sub-section-6-header-4">
          <h3>6.4 Real-World Testing</h3>
          <p>
            Real-world testing is a critical phase in validating the effectiveness of the search strategy, ensuring that the drones can perform as expected in a variety of conditions. Small, medium and full-scale tests were conducted throughout the development process to ensure the functionalities work as intended.
          </p>
          <p>
            Firstly, small-scale tests were conducted in E2A Studio 1 to optimize the MSBA parameters such as wall distance during wall-following. The algorithm to guide drones into the Unknown Search Area was also extensively tested by manually issuing radio command to the drones. 
          </p>
          <p>
            After integration with Mission Planner, the autonomous navigation of two drones to the Unknown Search Area could be tested. This enabled the optimization of drone orientation in the Start Area. It is crucial to ensure the drones can detect the Navigation Aids and reach the correct opening. In the first few tests, the drones were often mis-orientation, which created paths that deviated away from the Navigation Aids, resulting in the Unknown Search Area drones crossing into the Pillar Area. 
          </p>
          <p>
            Then, medium-scale tests were performed in E7 seminar room. With a larger space, two-third of the Unknown Search Area could be constructed, and all six Unknown Search Area drones could be tested simultaneously. However, due to some drones unable to stream images, a maximum of four drones was tested simultaneously across all the medium-scale tests. With a larger swarm drone, the tests revealed the weakness of this strategy – reliance on image streaming. Visual feedback is essential in guiding the drones into the Unknown Search Area and keeping them within the Unknown Search Area. During the tests, the Wi-Fi connection of the drones was intermittent, leading to windows of time without image feedback. When these periods overlap with drones approaching the Navigation Aid, this will result in them missing the entrances and entering the Known Search Area at the top of the Playing Field. This is problematic because it means the drones will not be able to search the Unknown Search Area, and in the worst case, create risk of collisions with the Known Search Area drones. 
          </p>
          <p>
            Understanding the significance of reliable Wi-Fi connectivity in executing the search strategy for Unknown Search Area, the team worked closely together to optimize the Wi-Fi network for the swarm drone and was able to mitigate the intermittent Wi-Fi connectivity. This was explained in detail in Section 3.5.
          </p>
          <p>
            Finally, full-scale tests were performed with the other subsystems in a competition-scale Playing Field. All six drones assigned to search the Unknown Search Area was tested. Through these tests, the positions of the Navigation Aids were optimized to guide the drones to the entrances while ensuring them to be detected to prevent the drones from exiting the area.  
          </p>
          <p>
            During the competition, one bonus victim was placed in the Unknown Search Area. The position of the bonus victim changes across the two runs. At the end, we were unable to rescue the one bonus victim in both runs.
          </p>
          <p>
            Table 8.0. Number of victims rescued in the Unknown Search Area
          </p>
          <image-component tag="image" source="./assets/section-6-unknown-area/table_8_unknown_results.png"
            subtitle=""></image-component>
          <p>
            In the first run, only one of the six MSBA drones successfully entered the Unknown Search Area through the top opening. During setup, one drone experienced issues streaming images back to the GCS and was retired before the run. At take-off, another drone failed to lift off. Of the four remaining drones that took off, two drifted significantly, leading to a mid-air collision. Among the two drones that remained airborne, one successfully entered the Unknown Search Area through the top opening, while the other deviated from the Navigation Aid at the right opening and crossed into the Pillar Area and Known Search Area.
          </p>
          <p>
            For the second run, to mitigate drifting during take-off, the extra April Tags brought to the competition were used as launch platforms by placing the drones on top of the AprilTags. While this method had been used by the previous team, our own testing showed it had negligible impact on take-off performance. The second run deployed only four out of six drones: the same drone that failed to stream images in the first run was retired again, and the drone that failed to take off was also removed. This adjustment created more vertical spacing between the drones, reducing the risk of mid-air collisions. With this configuration, two drones successfully entered the Unknown Search Area, while the remaining two flew past the Navigation Aids and veered into the Known and Pillar Area.
          </p>

        </div>

  
    </div>

      <div id="section-header-7" class="section" style="display: none;">
          <h2>7. Pillar Area (Matthew)</h2>
        <p>
          Another new feature of SAFMC 2025 is the addition of the Pillar Area. This area of the playing field has no Inner Walls and only Pillars to form the obstacles that the drones must navigate across.
        </p>
        <div id="sub-section-7-header-1">
          <h3>7.1 Introduction to the Pillar Area</h3>
        <p>
          In the Pillar Area, there are no Inner Walls within to navigate around. This would mean that a different strategy from that used in the Known and Unknown Search Areas would be needed for the drone to navigate it and search for Victim Markers.
        </p>

          <image-component tag="image" source="./assets/section-7-pillar-area/figure_7_1_pillar_area.png"
          subtitle="Figure 7.1: Pillar Area"></image-component>

        </div>
        <div id="sub-section-7-header-2">
          <br/>
          <h3>7.2 Mock-Up of the Pillar Area</h3>
        <p>
          To test the Pillar Area, we need to build a mock-up of 8 Pillars in the area for our drones to navigate through. This was achieved using Styrofoam boards, which were tested to ensure that the drones can detect them while flying and can also fly in a stable manner near them. Both were found to be satisfactory with our Crazyflie drones. The dimensions of the pillar are about 30cm in diameter, close to the competition specification of 40cm. The height of the pillar are about 30-40cm, tall enough for our initially planned flying altitude of 30cm.
        </p>
          <br/><br/>

          <image-component tag="image" source="./assets/section-7-pillar-area/figure_7_2_mock_pillars.png"
          subtitle="Figure 7.2: Mock Pillars Used in The Arena, Left Was Wrapped with Paper to Get the Right Pillar on The Right Image"></image-component>
          <br/>

          <image-component tag="image" source="./assets/section-7-pillar-area/figure_7_3_testing_pillar_area.jpg"
          subtitle="Figure 7.3: Testing Of Pillar Area in IDP Studio 1 (Mention Height of The Chair / Chair + Pillar Combo)"></image-component>
          <br/>Later, the flying height was adjusted to 80cm which was found to be more robust and reliable. To support this new flying height, pillars were placed on chairs as seen in Fig 7.3 that were about 75cm in height. The pillar would now be in the range of 75 – 115cm off the ground.

        </div>
        <div id="sub-section-7-header-3">
          <h3>7.3 Testing of Search Strategies Within</h3>
        <p>  
          Without Inner Walls to follow, the search strategy relies on using the Pillars to localise the path of the drones as it flies within the area. A few methods were considered:
          <br/>
          A.	Using the MSBA wall following algorithm, but with changes to its speed
          <br/>
          One method explored was to use the same MSBA wall following algorithm with the speed reduced to 0.2 m/s. This worked on the theory that the drone would interpret the Pillars as walls till it comes to free space again, where it would return to its original heading. However, it was found that the drones would follow the Pillar’s circumference as a wall and end up going in an infinite loop.
          <br/>
        </p>
          <image-component tag="image" source="./assets/section-7-pillar-area/figure_7_4_strat_A.png"
          subtitle="Figure 7.4: Description of Search Strategy A"></image-component>
          <br/>
        <p>
          B.	Using an algorithm that follows the preferred direction till meeting a Pillar, avoiding it by following the circumference
          <br/>
          This method would involve having the drone first fly forward (which is its preferred direction) until it encounters a Pillar head-on, at which time it would avoid the Pillar by flying in a circular path following the walls of the Pillar. It would then resume its original heading after doing so in the new lane that it has corrected its flight path to. However, it tends to drift and cut the Pillar while doing so.
          <br/>
        </p>
          <image-component tag="image" source="./assets/section-7-pillar-area/figure_7_5_strat_B.png"
          subtitle="Figure 7.5: Description of Search Strategy B"></image-component>
          <br/>
        <p>
          C.	Using an algorithm that follows the preferred direction till meeting a Pillar, and avoids by turning on the spot
          <br/>
          Similarly to method B, the drones would fly in the preferred direction until it meets a Pillar head on. However, unlike the previous method, it would turn on the spot to the left or right by 90 ° and move for a short distance before turning back by another 90 ° to resume its flight in the original heading. This would avoid it cutting the Pillar while doing its “avoidance”, and is a solution developed together with David Chong, an Undergraduate Research Experience (UREx) student working alongside us on our project.
          <br/>
        </p>
          <image-component tag="image" source="./assets/section-7-pillar-area/figure_7_6_strat_C.png"
          subtitle="Figure 7.6: Description of Search Strategy C"></image-component>
          <br/>
          <decision-matrix-pillar subtitle="Table 7.6: Comparison of three search strategies considered">
            <div id="decision-matrix-pillar"></div>
          </decision-matrix-pillar>
          </div>

        <div id="sub-section-7-header-4">
          <h3>7.4 Testing of Victim placement</h3>
          In the Pillar Area, targets were placed in different locations and the algorithm tested to ascertain the effectiveness of it to pick up the targets.
          <br/>
          1.	Victim Marker in free space
          <br/>
          Victim Marker are placed in the middle of the Pillar Area in free space. Drones are seen to be able to detect and rescue these victims
          <br/>
          2.	Target in front of the pillar
          <br/>
          Another configuration involved placing the target directly in front of the pillar. Similarly to the one in free space, the drones had no issues picking up the victims in front of the pillar.
          <br/>
        </p>
          <image-component tag="image" source="./assets/section-7-pillar-area/victim_front_pillar.png"
                           subtitle="Figure 7.4.2: Victim in front of the pillar"></image-component>
        <p>
          <br/>
          3.	Target behind the Pillar
          <br/>
          This was the hardest in theory to pick up since the drone would not be able to see the victim till it comes close to the target for the camera to pick it up. However, the drones were able to pick up and rescue the victims in the tests that we did. A failsafe however can be considered to increase the likelihood of rescuing the victim if it is not seen the first time that the drone passes through it.
          <br/>
        </p>
          <image-component tag="image" source="./assets/section-7-pillar-area/victim_behind_pillar.png"
                           subtitle="Figure 7.4.3: Victim behind the pillar"></image-component>
          <br/>
        </div>
        <div id="sub-section-7-header-5">
          <h3>7.5 Optimising for Search and Rescue</h3>
        <p>
          While Solution C was chosen as the ideal search strategy, further optimisations were made to maximise the coverage and reliability of the algorithm while minimising the search time. To achieve this, a few strategies were employed:
          <br/>
        </p>
          <div id="sub-section-7-header-5-header-1">
            <h4>7.5.1 Sliding to the Side When Near a Pillar</h4>
          <p>
            While the search strategy relies on avoiding a Pillar if it comes head-on to a Pillar, drift in the drone’s path can lead to it coming too close to Pillars on its side. A novel strategy was then employed for the drone to “slide” to the side should it come close to the Pillars, which would allow it to avoid collision with the Pillars while simultaneously maintaining the original heading of its flight path.
          </p>
            <video-component tag="video" source="https://www.youtube.com/embed/X_Eua5u_tts?si=fUMhbO6se7Xcpaq0" subtitle="Figure 7.4: Video of drone sliding to the side when too close to the Pillar"></video-component>
          </div>

          <div id="sub-section-7-header-5-header-2">
            <h4>7.5.2 Optimising the speed of the drones</h4>
          <p>
            Another parameter that we sought to optimise was the speed of the drones while flying within the pillar zone. Flying faster would mean that we are able to complete the mission in a quicker time, but would also increase the chance of crashing into pillars due to either slow or non-detection of the pillars while flying. This happens as the drones do require a processing time between detecting the pillars and avoiding it, and flying too fast would mean that the drones would crash into pillars before they are able to avoid it. Flying too slowly however would cause the mission to take too much time to complete.
            <br/>
            A speed of 0.5m/s was initially chosen. However, this was too fast leading to frequent crashes with drones. 0.2m/s was found to work well, but too slow. 0.25m/s was later found to be the optimal speed balancing speed with reliability of the algorithm.
            <br/>
          </p>
          </div>

          <div id="sub-section-7-header-5-header-3">
            <h4>7.5.3 Navigation Aid to Reverse at the End of the Pillar Area</h4>
          <p>
            Given the Pillar drones are optimised to carry out search and rescue in the Pillar Area, it should spend the duration of the mission searching the Pillar Area instead of drifting to other zones. Navigation Aids are therefore used at the end of the Pillar Area to send a command to reverse, which is like that employed in the Known Search Area. This would keep the drones “within” this area where it is best suited to search and capture targets that may have been placed directly behind a Pillar and missed in the first run.
          </p>
            <image-component tag="image" source="./assets/section-7-pillar-area/figure_7_8_reverse.png"
            subtitle="Figure 7.8: Reverse"></image-component>

          </div>
          <div id="sub-section-7-header-5-header-4">
            <h4>7.5.4 Straight line vs Zig-Zag path</h4>
          <p>
            The last parameter that we considered was whether the drone should fly in a straight-line path or a zig-zag one (as seen in the images below):
            <br/>
          </p>
            <image-component tag="image" source="./assets/section-7-pillar-area/straight-vs-zigzag.png"
                             subtitle="Figure 7.9: Straight-line vs Zig-zag flight path"></image-component>
            <br/>
          <p>
            By following a zig-zag path, we can increase the field of view of the camera with an intent to increase the reliability of target detection. However, this comes at the cost of more frequent crashes and an unpredictable flight path. After our testing, we found that this was not a good change and a straight line path would be more desirable given the trade-offs.
          </p>
          </div>
        </div>

        <div id="sub-section-7-header-6">
          <h3>7.6 Full Algorithm</h3>
        <p>
          Putting all together, we now have the following algorithm for search and rescue within the Playing Field for a single drone.
          <br/>
          <image-component tag="image" source="./assets/section-7-pillar-area/figure_7_9_1_full_algo.png"
          subtitle="Figure 7.10: Full Algorithm"></image-component>
          <br/>
        </p>
        <p>
          Based on our testing as seen in Figure 7.11, 3 drones flying with this algorithm are needed to achieve full search coverage of the pillar area while remaining robust and reliable. Any fewer drones would cause some search areas to be left uncovered, while allocating more drones may increase the chance of drone-to-drone collisions which would be counterproductive.
          <br/>
        <p>
          <image-component tag="image" source="./assets/section-7-pillar-area/figure_7_9_2_full_algo.png"
          subtitle="Figure 7.11: Full Algorithm"></image-component>
          <br/>
        <p>
          A video of the pillar test with 3 drones can be seen here:
        </p>
          <br/>
          <br/>
          <video-component tag="video" source="https://www.youtube.com/embed/3jxxZiSw0ME?si=RA4qgXhqMRl-FVWZ" subtitle="Figure 7.12: Video of pillar test with 3 drones"></video-component>
        </div>
    </div>

      <br />
      <br />

      <div id="section-header-8" class="section" style="display: none;">
        <h2>8. Mission Planning (Shu Hui)</h2>
        <p>
          Mission Planning subsystem involves the efficient integration of the subsystems the SAFMC 2025 team have developed into a robust mission execution plan for our drone swarm. It encompasses (1) the optimisation of take-off sequence, (2) the optimisation of kill and landing sequence for mission termination and (3) managing the communication between GCS and the drones.
        </p>

    <div id="sub-section-8-header-1">
        <h3>8.1 Optimisation of Take-off Sequence</h3>

        <image-full tag="image" source="./assets/section-8-mission-planning/figure_8_1_take_off_sequence.png"
        subtitle="Figure 8.1: Take-off Sequence"></image-full>

        <p>
          Drones assigned to channel 60 will take-off first, followed by drones assigned to channel 80 after a 15 s delay (Figure 8.1). 15 s was chosen to clearly differentiate the take-offs of the group of drones and to prevent collisions and overcrowding at the Start Area. As previously mentioned, with a limited number of Crazyradio dongles connected to GCSs, the 15 s allows for time to minimise radio traffic by the Crazyradio dongles.
        </p>

        <p>
          As competition rule book stipulates a maximum of two simultaneous take-off of drones and a 10 second window between take-offs, the drones were either assigned a channel of 60 or 80 for simplicity, unlike previous years where they used multiple channels. Using more channels was possible but, each additional channel used results in 3 extra packets sent to the Crazyflies of that assigned channel. Given hardware limitations and heavy reliance on commands sent via CPX protocol this year, we decided to limit the number of channels to reduce unnecessary packet transmission.
        </p>
        <p>
          Known Search Area is the largest area in the Playing Field and have the highest concentration of Regular Victims, so it makes sense to assign ten drones to the Known Search Area to optimise Mission Score. The Known Search Area drones was arranged in a manner that optimises the initial coverage and time used for search.
        </p>
        <p>
          Unknown Search Area contains the same type of Obstacles as the Known Search Area but contains only Bonus Victim(s). Since the layout of the Unknown Search Area was not disclosed, but we needed to optimise coverage, a moderate number of six drones was assigned to Unknown Search Area. The Unknown Search Area drones can only enter the Unknown Search Area via two entrances/exits, thus they are placed in a straight line, each group of drones during the different take-offs will enter via different entrances/exits. Once inside the Unknown Search Area, the drones will carry out their respective missions.
        </p>
        <p>
          Pillar Area contains Bonus Victim(s) and Pillar obstacles. Given the limitation of the multi-ranger deck’s ToF sensors and the narrow 1 m Pillar-to-Pillar distance, an effective Pillar Area search strategy was developed. A small number of drones is assigned to the area to minimise the risk of drone-to-drone collisions and collisions with the Pillars while complementing the search approach. The Pillar Area drones was also placed in a straight line for sequential entry. Upon entering the Pillar Area, the drones will carry out their respective missions.
        </p>
    </div>

    <div id="sub-section-8-header-2">
        <h3>8.2 Optimisation of Kill and Landing Sequence</h3>
        <p>
          Competition rule book stipulates the implementation of a failsafe capability in the event of a loss of link. The SAFMC 2024 team developed a <i>kill.py</i> script which completely stops the motors of a Crazyflie after specifying the channel and drone number when the script is executed.
        </p>
        <p>
          The SAFMC 2025 team developed a <i>kill_all.py</i> script (CDE-4301-ASI-401, n.d.) which completely stops the motors of all the Crazyflies that are turned on within 1 km radius (ideal condition for Crazyradio), regardless of channel number and status. In the event of catastrophic failure such as link loss or GCS crash where either the Wi-Fi routers are malfunctioning or are turned off, image streaming will not work and due to the Crazyflies’ small size, and it would be impossible to tell which malfunctioning drones are still flying and in which areas. The safest option is to stop all the drones within the vicinity immediately.
        </p>
        <p>
          The mission termination criteria is set at 400 s (CDE-4301-ASI-401, n.d.) or when all the Victim Markers are rescued. The mission time was determined by allowing the Crazyflie to fly around with a fully charged battery several times and measuring the time until it lost power. Under optimal conditions, with a battery voltage of 4.20 V, the expected mission time is around 400 s. This means that for the full duration of the drones’ flight time, we can track mission progress by printing the elapsed time (CDE-4301-ASI-401, n.d.). However, to account for the situation where the drones may be lost in the Playing Field and cannot rescue all the Victims, the <i>land_all.py</i> developed by the SAFMC 2024 team can be used to end the mission prematurely.
        </p>
    </div>

    <div id="sub-section-8-header-3">
      <h3>8.3 Communication Between the Ground Control Station and Drones</h3>

        <p>
          With reference to Figure 8.2, The ESP32 chip on the AI Deck streamed compressed JPEG images via TCP sockets over Wi-Fi directly to the drone's assigned GCS.
        </p>
        <p>
          Task allocation, take-off, landing, and behaviour-triggering commands based on GCS processing of images or were sent from the GCSs to the drones using the CPX protocol via the dynamically managed pool of Crazyradio dongles.
        </p>
        <image-full tag="image" source="./assets/section-8-mission-planning/figure_8_2_communication_gcs_drones.png"
        subtitle="Figure 8.2: Communication Between the GCS and Drones"></image-full>
        <p>
          The detection process (Figure 8.3) begins with a take-off command issued via CPX from the GCS, followed by a check of the drone’s status to ensure it is ready for the mission. Once in the air, the drone continuously scans its surroundings for AprilTags, while continuously streaming image to the GCS via Wi-Fi. The GCS processes the image of the detected AprilTag and classifies the detected AprilTags as a Victim Marker or a Danger Zone or Navigation Aid.
        </p>
        <p>
          If the GCS “recognizes” a Victim Marker, it marks the Victim Marker as "Detected" in the Mission Planner. The GCS will then issue a land command via CPX to the drone and allowing the drone to land in front of the Victim Marker. Conversely, if the detected AprilTag is a Danger Zone, the GCS records the ID of that Danger Zone. The drone continues to fly and avoids landing in that area. If a Navigation Aid is detected, the GCS triggers a behaviour (reverse, turn, change search pattern) by sending appropriate CPX commands for that Navigation Aid.
        </p>
        <p>
          If no AprilTags are detected during the flight, the drone keeps scanning its environment until the GCS identifies an AprilTag.
        </p>
        <image-full tag="image" source="./assets/section-8-mission-planning/figure_8_3_detection_chart.png"
        subtitle="Figure 8.3: Detection Chart"></image-full>

  </div>

  </div>

  <br />
  <br />

      <div id="section-header-9" class="section" style="display: none;">
          <h2>9. Full-Scale Systems Testing</h2>


      <div id="sub-section-9-header-1">
          <h3>9.1 Testing in Residential College 4 Multi-Purpose Sports Hall</h3>
        <p>
        To ascertain the accuracy and reliability of our overall search strategy, we conducted our full-scale testing at the Multi-Purpose Sports Hall of Residential College 4 (RC4 MPSH). RC4 MPSH’s dimensions was about 20 m x 20 m, providing a good approximation of the actual competition layout. This was also the first chance for us to test the search strategies for the Known, Unknown and Pillar search Areas all at once.
        </p>
        <br/><br/>

        <image-component tag="image" source="./assets/section-9-full-scale-testing/figure_9_1_full_scale_rc4.jpg"
        subtitle="Figure 9.1: Full-Scale Testing Carried Out at RC4 MPSH"></image-component>
        <br/>

        <p>
        In the RC4 MPSH, we first started by testing the algorithms without image streaming (no target detection). This allowed us to ascertain the feasibility of our search strategies and collision avoidance first.
        Later, we had a total of three runs testing the overall performance of the drone swarm with image streaming on to enable target detection and rescue of Victim Markers. The score of our three runs were:
        <br/><br/>
        </p>

        <rc4-test-scores subtitle="Table 9.1: Test Scores in RC4">
          <div id="rc4-test-scores"></div>
        </rc4-test-scores>

        <br/>

        <image-component tag="image" source="./assets/section-9-full-scale-testing/pillar_victim_rescued.png"
        subtitle="Figure 9.2: Victim rescued in the pillar zone"></image-component>
        <br/>
        <image-component tag="image" source="./assets/section-9-full-scale-testing/unknown_victim_rescued.png"
                         subtitle="Figure 9.2: Victim rescued in the unknown area"></image-component>
        <br/>
      <p>
        Run 3 was the best of all runs taken, where three Victim Markers were rescued (of which one is bonus) while none of the Danger Zones were within 1m of the drones after landing.
        <br/>
        Originally, we were not able to rescue any victims and score points as we faced a range of issues that hindered our drone swarm’s performance. These are described more in detail below, including how they were resolved as we improved in each run.
        <br/>
      </p>

      <div id="sub-section-9-header-1-header-1">
        <h4>9.1.1 Network and Bandwidth Issues</h4>
      <p>
        The original plan was to set up one central computer to manage all drones in the Playing Field, and it would do so on one Wi-Fi network with 3 routers. This would be up to 20 drones managed on one central system. While this was not an issue with individual system testing using up to 10 drones, it did not perform well with 20 at once which could have been due to overloading of the bandwidth. Drones would often either not connect to the Wi-Fi or do so then disconnect mid-flight. Once the Wi-Fi disconnects, the drones can no longer send images from the camera to the central system to process which means that it would not be able to rescue any victims.
        <br/>
        Eventually, a decision was made to change de-link the network into 3 separate components that would be controlled by 3 separate computers. This worked much better and gave us the assurance that the system would be reliable moving into the competition day.
      </p>

      </div>

      <div id="sub-section-9-header-1-header-2">
        <h4>9.1.2 Different Performance of the Drones in Different Conditions</h4>
        
      <p>
        Moving to RC4 MPSH, there were a few changes in conditions. First, the floor posed new challenges as it was shinier than the one seen in our IDP studio. This led to undefined behaviour of the drones, including speed-ups and inconsistent altitude observed. The drones were therefore not able to rescue Victim Markers or did so inconsistently, given that the speed varied drastically mid-flight.
        <br/>
        To counter this, we had to vary the distance of the drones to ensure the stable performance of them within the RC4 MPSH. By flying them higher, this would allow us to counter the effect of the reflective surface given that the drones are further from the ground.  By increasing the altitude from the original 30 cm to 80 cm, the drones were seen to perform in a much more stable manner. It could then fly in a much more predictable fashion allowing a more reliable search and rescue.
        <br/>
      </p>

      </div>


      <div id="sub-section-9-header-1-header-3">
        <h4>9.1.3 Overall Analysis</h4>
      <p>
        The conditions faced at RC4 MPSH were starkly different from that seen in individual component testing both due to the environment as well as the scale. It was therefore fortunate that we started the process early in Week 4, going to RC4 MPSH for a total of 4 times, which allowed us ample time to resolve the issues faced before going for the competition. The conditions at the actual competition, as described in the next paragraph, were found to be highly like that in the test here.
        <br/>

      </p>
      </div>

      </div>

      <div id="sub-section-9-header-2">
          <h3>9.2 Competition Day</h3>

        <p>
          On the 17th of March 2025, our team participated in SAFMC 2025 held at the Singapore Expo. This was the “final” full scale test with our full system being run all together.
        <br/>
        </p>

        <image-component tag="image" source="./assets/section-9-full-scale-testing/setup.png"
                         subtitle="Figure 9.2: Set up @ SAFMC 2025"></image-component>
        <br/><br/>
        
      <p>
        We were given two runs to test our drone swarm system with the better of the two runs taken as the final score. A total of 5 Victim Markers (of possible 8 as given in the rules) were used on the actual day, and likewise 3 Danger Zones (of possible 4) were used. The location of the Victim Markers and Danger Zones were not made known to us before placing our drones down and were only placed in the Playing Field after deciding on the placement of drones in the Playing Field. We only knew that the two Bonus Victims would be placed in the Pillar and Unknown Search Areas (one each) while the rest of the Regular Victims could be placed anywhere in the Playing Field.
        <br/><br/>
      </p>

        <image-component tag="image" source="./assets/section-9-full-scale-testing/actual-pillar.png"
                         subtitle="Figure 9.3: Pillar Area in the actual arena"></image-component>
       <p> 
        <br/><br/>
        In the actual Playing Field, the performance of our drone swarm was like that exhibited on our final run at RC4 (conducted one week prior to the event) but with greater stability as the carpeted floor allowed the drones to fly in a much more stable manner. The results of the two runs are as follows:
        <br/><br/>
      </p>

        <actual-test-scores subtitle="Table 9.3: Actual test scores @ SAFMC 2025">
          <div id="actual-test-scores"></div>
        </actual-test-scores>
        <br/><br/>
        Run 2 was taken as the final score given that it was the better of two runs. Given that we were able to rescue all but one Victim marker, we can conclude that our test environment was a good gauge of the actual Playing Field, with our search strategies and target detection working as intended in the actual competition.
        <br/><br/>
      </p>
        
        <image-component tag="image" source="./assets/section-9-full-scale-testing/team_photo.jpg"
                         subtitle="Figure 9.2: Successful SAFMC 2025!"></image-component>
      </div>
    </div>

    <br />
    <br />

      <div id="section-header-10" class="section" style="display: none;">
          <h2>10. Conclusion and Future Work</h2>
          <p>
            Using Bitcraze’s Crazyflie as the drone platform, our team developed a 20-drone swarm for an indoor search and rescue mission in Category E of SAFMC 2025. To detect objects such as Victim Markers, Danger Zones, and Navigation Aids, the AI deck was integrated to provide the drones with visual capabilities. To optimize the search process, the 20 m by 20 m search arena was divided into three distinct regions: the Known Search Area, Unknown Search Area, and Pillar Area, enabling the development of specialized strategies tailored to each zone.
          </p>
          <p>
            While the overall drone swarm was effective in its search operation in the competition arena, several challenges were encountered. Addressing these limitations presents opportunities for future improvements, enhancing the robustness and performance of the system in terms of time, reliability and coverage.
          </p>
        
        <div id="sub-section-10-header-1">
            <h3>10.1 Expanding Drone Capabilities</h3>
          <p>
            The key disadvantage our team face is that our current drone lacks sufficient space for additional expansion decks, which limits our exploration opportunities. Yet, due to the critical functional requirements our current decks fulfils for autonomous flight and navigation, we cannot remove decks that we currently use. As such, an alternative approach is to use the BigQuad deck (Figure 10.1) (Bitcraze, n.d.) to build a larger platform that can accommodate more expansion decks. This would provide greater flexibility for integrating additional sensors and modules, enhancing the drone’s overall functionality. With the increased capacity on the Crazyflie, we could explore the prototyping deck (Figure 10.2)(Bitcraze, n.d.), which allows for the integration of custom hardware, including the ability to solder a Bluetooth chip on it and other capacities (See Section 10.2). 
          </p>

          <image-component tag="image" source="./assets/section-10-future-work/figure_10_1_big_quad.jpg"
          subtitle="Figure 10.1: Big Quad Deck"></image-component>

          <image-component tag="image" source="./assets/section-10-future-work/figure_10_2_prototyping_deck.jpg"
          subtitle="Figure 10.2: Prototyping Deck"></image-component>

        </div>

        <div id="sub-section-10-header-2">
          <h3>10.2. Decentralized Distributed SLAM </h3>
        <p>
          Based on our experience and observations of other teams during the competition, decentralized solutions significantly enhance the reliability of a drone swarm. Implementing a mapped solution for the Unknown Search Area greatly improves search coverage. However, one limitation that prevented us from adopting such a strategy was hardware constraints. The ETH-PBL lab overcame the poor performance of SLAM by procuring a custom ToF sensor deck and implementing state-of-the-art SLAM features, such as loop closure detection.
        </p>
          
        <p>
          By adopting ETH’s custom hardware (Figure 10.3 and Figure 10.4) and SLAM solution (ETH-PBL, n.d.) the team would gain access to a wider range of algorithms for optimizing search strategies. For example, multi-agent frontier search could be implemented to improve exploration of the Unknown Search Area. Additionally, having an accurate map enables adaptive search strategies, allowing drones to assess real-time coverage, adjust their paths dynamically, and coordinate more effectively to avoid redundant exploration.
        </p>

        <image-full tag="image" source="./assets/section-10-future-work/figure_10_3_eth_1.png"
        subtitle="Figure 10.3: ETH Implementation"></image-full>

        <image-component tag="image" source="./assets/section-10-future-work/figure_10_4_eth_2.jpg"
        subtitle="Figure 10.4: ETH Implementation"></image-component>
        
        <p>
          Furthermore, with a mapped solution, drones would be able to navigate to the Unknown Search Area more reliably without relying on a series of pre-planned motions that need to be executed precisely. Instead of depending on fixed movement sequences that need to be execute timely and precisely, drones could make real-time adjustments based on their perceived environment, reducing the risk of errors due to minor deviations or unexpected obstacles. This would also make the system more robust against disturbances, such as drift, communication delays, or intermittent sensor data, ultimately improving overall mission success.
        </p>
        <p>
          However, implementing this solution requires sufficient space on the drone to accommodate the additional deck.
        </p>
      </div>
      </div>

    <br />
    <br />

      <div id="appendices" class="section" style="display: none;">
          <h2>Appendix A: Decision Matrix for Selection of Drone Development Platform</h2>

          <drone_platform_selection subtitle="">
            <div id="drone_platform_selection"></div>
          </drone_platform_selection>    

          <p> 
            The selection of drone platform was based on six equal-weighed key factors, size and weight, hardware modularity, software modularity, sensor capability, cost, and parts availability. For each factor, the drone platform is given a score out of 5, the higher the better.           
          </p>

          <ul style="list-style-type: decimal; padding: 0; margin: 0; text-align: justify;">
            <li>
              A smaller and lighter drone enhances manoeuvrability, especially in confined spaces, and improves swarm scalability. Among the evaluated platforms, the Bitcraze Crazyflie is the smallest and lightest, followed by the DJI Tello, which has a slightly larger size and higher weight.
              <br></br>
            </li>
            <li>
              A modular hardware design allows for easy upgrades and component replacements. Higher scores indicate greater flexibility in adding or replacing components. Crazyflie and DEXI are highly modular, as both suppliers provide spare parts and easily swappable components. DEXI scores higher since certain parts, like its onboard computer (Raspberry Pi), are off-the-shelf, whereas Bitcraze relies on a custom PCB. DJI Tello, in contrast, only offers spare parts for specific components like propellers and propeller guards.
              <br></br>
            </li>
            <li>
              Open-source software and compatibility with frameworks like ROS are crucial to provide better software flexibility, developer support, and ease of integration. Crazyflie and DEXI both have high scores as both have open-source software. However, DEXI ranks higher as its on-board computer (Raspberry Pi) provides greater processing power and flexibility compared to Crazyflie’s STM microcontroller. DJI Tello has a low score of 2 due to it running on a proprietary software.
              <br></br>
            </li>
            <li>
              The drone must support onboard sensors for localization, obstacle detection, and avoidance. A higher score is given to platforms with more built-in or easily integrable sensors.  Crazyflie and DEXI have high scores as both offer the same sensor suite such as optical flow sensors, multi-TOF distance sensors, camera etc. On the other hand, DJI Tello only contains a camera module, limiting its capabilities to vision-based solutions.
              <br></br>
            </li>
            <li>
              The cost of each platform should be reasonable as they will be scaled up to form a swarm. In terms of pricing per unit, DJI Tello is the most affordable at USD 109, while Crazyflie, including the additional sensor decks cost USD 640. The DEXI Level III is the most expensive at USD 2000.
              <br></br>
            </li>
            <li>
              Readily available drones and spare parts ensure ease of maintenance. DJI and Bitcraze both have authorized retailers in Singapore, making their spare parts highly accessible. However, DEXI only operate in the United States, requiring international shipping for spare parts, which leads to increased lead time and cost.
              <br></br>
            </li>
          </ul>

      </div>

    <br />
    <br />
 
    <div id="references" class="section" style="display: none;">
      
      <h2>References</h2>
      <ul style="text-align: justify;">

        <li>
          Aideck-gap8-examples/examples/other/wifi-img-streamer at master · bitcraze/aideck-gap8-examples. (n.d.). GitHub. Retrieved 29 March 2025, from https://github.com/bitcraze/aideck-gap8-examples/tree/master/examples/other/wifi-img-streamer
        </li>
        <li>
          AprilRobotics/apriltag. (2025). [C]. AprilRobotics. https://github.com/AprilRobotics/apriltag
        </li>
        <li>
          BigQuad deck | Bitcraze. (n.d.). Retrieved 2 April 2025, from https://www.bitcraze.io/products/bigquad-deck/
        </li>
        <li>
          BigQuad deck | Bitcraze. (n.d.). Retrieved 2 April 2025, from https://www.bitcraze.io/products/bigquad-deck/
        </li>
        <li>
          Comparison of fiducial markers. (2022, April 26). Robotics Knowledgebase. https://roboticsknowledgebase.com/wiki/sensing/fiducial-markers/
        </li>
        <li>
          ETH-PBL. (n.d.). Nano Swarm Mapping [GitHub repository]. GitHub. Retrieved April 3, 2025, from https://github.com/ETH-PBL/Nano_Swarm_Mapping
        </li>
        <li>
          Freertos+posix—FreertosTM. (n.d.). Retrieved 29 March 2025, from https://freertos.org/Documentation/03-Libraries/05-FreeRTOS-labs/03-FreeRTOS-plus-POSIX/00-FreeRTOS-Plus-POSIX
        </li>
        <li>
          GitHub—Williamleong/aideck-gap8-examples at dev/william. (n.d.). GitHub. Retrieved 29 March 2025, from https://github.com/williamleong/aideck-gap8-examples
        </li>
        <li>
          Github—Cde-4301-asi-401/missionplanner. (n.d.). GitHub. Retrieved 29 March 2025, from https://github.com/CDE-4301-ASI-401/MissionPlanner
        </li>
        <li>
          Loco explorer bundle—Crazyflie 2.1+. (n.d.). Bitcraze Store. Retrieved 29 March 2025, from https://store.bitcraze.io/products/loco-explorer-bundle
        </li>
        <li>
          Maximum range for loco positioning | Bitcraze. (n.d.). Retrieved 29 March 2025, from https://www.bitcraze.io/documentation/system/positioning/max-range-loco/
        </li>
        <li>
          MissionPlanner/mission_planner/kill_all.py at main · CDE-4301-ASI-401/MissionPlanner. (n.d.-a). GitHub. Retrieved 29 March 2025, from https://github.com/CDE-4301-ASI-401/MissionPlanner/blob/main/mission_planner/kill_all.py
        </li>
        <li>
          MissionPlanner/mission_planner/mission_script.py at main · CDE-4301-ASI-401/MissionPlanner. (n.d.). GitHub. Retrieved 29 March 2025, from https://github.com/CDE-4301-ASI-401/MissionPlanner/blob/main/mission_planner/mission_script.py
        </li>
        <li>
          Prototyping deck | Bitcraze. (n.d.). Retrieved 2 April 2025, from https://www.bitcraze.io/products/prototyping-deck/
        </li>
        <li>
          Prototyping deck | Bitcraze. (n.d.). Retrieved 2 April 2025, from https://www.bitcraze.io/products/prototyping-deck/
        </li>
        <li>
          Rauch, C. (2025). Christianrauch/apriltag_msgs [CMake]. https://github.com/christianrauch/apriltag_msgs
        </li>
        <li>
          Rauch, C. (2025). Christianrauch/apriltag_ros [C++]. https://github.com/christianrauch/apriltag_ros
        </li>
        <li>
          Release 2025.02 · bitcraze/aideck-esp-firmware. (n.d.). GitHub. Retrieved 29 March 2025, from https://github.com/bitcraze/aideck-esp-firmware/releases/tag/2025.02
        </li>
        <li>
          Tl-nus-cfs/ai_deck_wrapper. (2023). [Python]. TL@NUS Centre For Flight Sciences. https://github.com/TL-NUS-CFS/ai_deck_wrapper
        </li>
         
      </ul>
    </div>

  </div>
  </div>

  <sl-button class="scroll-to-top" variant="primary" size="medium" circle onclick="scrollToTop()">
    <sl-icon name="arrow-up" label="Settings"></sl-icon>
  </sl-button>

  <script src="https://unpkg.com/gridjs/dist/gridjs.umd.js"></script>
  
  <script type="module" src="./components/table-component/table-component.js"></script>

  <script src="showhidefunction.js"></script>

</body>

</html>
